from transformers import (
    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,
    DataCollatorForLanguageModeling, TrainerCallback, EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset
import torch
import numpy as np
from sklearn.metrics import accuracy_score
import evaluate

# é…ç½®å‚æ•°
model_path = "./models"  # ä»é¡¹ç›®æ ¹ç›®å½•çš„ models ç›®å½•åŠ è½½åŸºç¡€æ¨¡å‹
output_dir = "./models/qwen-ai-girlfriend-lora"
dataset_path = "./data/train/girlfriend_chat_dataset_20251117_055552.json"  # 2000æ¡è®­ç»ƒæ•°æ®

print("åŠ è½½æ¨¡å‹å’Œtokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# æ·»åŠ pad tokenå¦‚æœä¸å­˜åœ¨
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# é’ˆå¯¹ MPS ä¼˜åŒ–æ¨¡å‹åŠ è½½
if torch.backends.mps.is_available():
    print("æ£€æµ‹åˆ°MPSè®¾å¤‡ï¼Œä½¿ç”¨Mac GPUåŠ é€Ÿ")
    # ä½¿ç”¨ float16 å‡å°‘å†…å­˜å ç”¨ï¼Œé¿å…å¡æ­»
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        dtype=torch.float16,  # ä½¿ç”¨float16ï¼Œå†…å­˜å ç”¨å‡åŠ
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    model = model.to("mps")
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    model.gradient_checkpointing_enable()
    print("å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨float16ç²¾åº¦")
else:
    print("ä½¿ç”¨CPUæˆ–CUDAè®¾å¤‡")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model.gradient_checkpointing_enable()

# ä¼˜åŒ–çš„LoRAé…ç½® - å¹³è¡¡æ€§èƒ½ä¸å†…å­˜
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=4,  # ä¿æŒé€‚ä¸­çš„ç§©ï¼Œå¹³è¡¡æ•ˆæœä¸å†…å­˜
    lora_alpha=16,  # ç›¸åº”è°ƒæ•´alpha
    lora_dropout=0.15,  # é€‚åº¦dropout
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # è®­ç»ƒæ ¸å¿ƒæ³¨æ„åŠ›æ¨¡å—
    bias="none"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# æ•°æ®é¢„å¤„ç† - æ·»åŠ æ•°æ®å¢å¼º
def preprocess_function(examples):
    texts = []
    for i in range(len(examples['instruction'])):
        # éšæœºå†³å®šæ˜¯å¦åŒ…å«systemæŒ‡ä»¤ï¼Œå¢åŠ æ•°æ®å¤šæ ·æ€§
        if np.random.random() > 0.2:  # 80%çš„æ¦‚ç‡åŒ…å«systemæŒ‡ä»¤
            messages = [
                {"role": "system", "content": examples['instruction'][i]},
                {"role": "user", "content": examples['input'][i]},
                {"role": "assistant", "content": examples['output'][i]}
            ]
        else:
            # 20%çš„æ¦‚ç‡ä¸åŒ…å«systemæŒ‡ä»¤ï¼Œè®©æ¨¡å‹å­¦ä¹ åœ¨æ²¡æœ‰ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹å›åº”
            messages = [
                {"role": "user", "content": examples['input'][i]},
                {"role": "assistant", "content": examples['output'][i]}
            ]

        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        texts.append(text)

    tokenized = tokenizer(
        texts,
        truncation=True,
        max_length=384,  # é€‚åº¦å‡å°‘åºåˆ—é•¿åº¦ï¼Œä¿ç•™è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡
        padding=False,
        add_special_tokens=True
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized


# åŠ è½½å¹¶åˆ†å‰²æ•°æ®
dataset = load_dataset('json', data_files=dataset_path)

# å¯¹å°æ•°æ®é›†è¿›è¡Œè®­ç»ƒ/éªŒè¯åˆ†å‰²
if len(dataset['train']) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®åˆ†å‰²
    dataset = dataset['train'].train_test_split(
        test_size=0.2,  # 20%ä½œä¸ºéªŒè¯é›†
        shuffle=True,
        seed=42
    )
else:
    # å¦‚æœæ•°æ®å¤ªå°‘ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒï¼Œåˆ›å»ºä¸€ä¸ªå°çš„è™šæ‹ŸéªŒè¯é›†
    train_dataset = dataset['train']
    # å–å‰10æ¡ä½œä¸ºéªŒè¯é›†ï¼ˆå¦‚æœæ•°æ®å¾ˆå°‘ï¼‰
    if len(train_dataset) > 20:
        val_size = min(10, len(train_dataset) // 5)
        dataset = train_dataset.train_test_split(
            test_size=val_size,
            shuffle=True,
            seed=42
        )
    else:
        # æ•°æ®éå¸¸å°‘ï¼Œå…¨éƒ¨ç”¨äºè®­ç»ƒ
        dataset = {'train': train_dataset, 'test': train_dataset.select(range(min(3, len(train_dataset))))}

print(f"è®­ç»ƒé›†å¤§å°: {len(dataset['train'])}")
print(f"éªŒè¯é›†å¤§å°: {len(dataset['test'])}")

tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset['train'].column_names
)


# è®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    # è®¡ç®—å‡†ç¡®ç‡ï¼ˆå¿½ç•¥padding tokenï¼‰
    mask = labels != -100
    aligned_predictions = predictions[mask]
    aligned_labels = labels[mask]

    accuracy = accuracy_score(aligned_labels, aligned_predictions)

    # è®¡ç®—perplexityï¼ˆéœ€è¦äº¤å‰ç†µæŸå¤±ï¼‰
    loss_fct = torch.nn.CrossEntropyLoss()
    logits_tensor = torch.tensor(logits)
    labels_tensor = torch.tensor(labels)

    # ç§»é™¤éæ ‡ç­¾ä½ç½®
    mask = labels_tensor != -100
    active_logits = logits_tensor[mask]
    active_labels = labels_tensor[mask]

    loss = loss_fct(active_logits, active_labels)
    perplexity = torch.exp(loss)

    return {
        "accuracy": accuracy,
        "perplexity": perplexity.item(),
        "loss": loss.item()
    }


# è®­ç»ƒå›è°ƒ - æ·»åŠ å†…å­˜ç®¡ç†
class TrainingMonitor(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and 'loss' in logs:
            print(f"Step {state.global_step}: Loss = {logs['loss']:.4f}")
        if logs and 'eval_loss' in logs:
            print(f"Step {state.global_step}: Eval Loss = {logs['eval_loss']:.4f}")
        
        # å®šæœŸæ¸…ç† MPS ç¼“å­˜
        if state.global_step % 10 == 0 and torch.backends.mps.is_available():
            torch.mps.empty_cache()

    def on_epoch_begin(self, args, state, control, **kwargs):
        print(f"\nğŸš€ å¼€å§‹ç¬¬ {state.epoch} è½®è®­ç»ƒ")
        # æ¯è½®å¼€å§‹å‰æ¸…ç†ç¼“å­˜
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
            print("å·²æ¸…ç† MPS ç¼“å­˜")

    def on_epoch_end(self, args, state, control, **kwargs):
        print(f"âœ… å®Œæˆç¬¬ {state.epoch} è½®è®­ç»ƒ")
        # æ¯è½®ç»“æŸåæ¸…ç†ç¼“å­˜
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
            print("å·²æ¸…ç† MPS ç¼“å­˜")


# ä¼˜åŒ–çš„è®­ç»ƒå‚æ•° - é’ˆå¯¹å°æ•°æ®é›†å’Œå†…å­˜é™åˆ¶
training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    num_train_epochs=5,  # å¢åŠ è½®æ•°ä½†ä½¿ç”¨æ—©åœ
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,  # å‡å°‘ç´¯ç§¯æ­¥æ•°ï¼Œé™ä½å†…å­˜å‹åŠ›
    warmup_ratio=0.15,  # æ›´é•¿çš„é¢„çƒ­æœŸï¼Œç¨³å®šè®­ç»ƒ
    logging_steps=5,  # æ›´é¢‘ç¹çš„æ—¥å¿—
    eval_steps=20,  # å®šæœŸè¯„ä¼°
    save_steps=100,
    learning_rate=2e-5,  # æ˜¾è‘—é™ä½å­¦ä¹ ç‡ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§
    fp16=False,  # åœ¨MPSä¸Šç¦ç”¨fp16è®­ç»ƒï¼Œé¿å…nan
    remove_unused_columns=True,
    report_to=None,
    dataloader_pin_memory=False,
    save_strategy="steps",
    logging_strategy="steps",
    eval_strategy="steps",  # å¯ç”¨è¯„ä¼°
    load_best_model_at_end=True,  # è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡å‹
    metric_for_best_model="eval_loss",  # æ ¹æ®éªŒè¯æŸå¤±é€‰æ‹©æœ€ä½³æ¨¡å‹
    greater_is_better=False,  # æŸå¤±è¶Šå°è¶Šå¥½
    prediction_loss_only=False,  # éœ€è¦è®¡ç®—å®Œæ•´æŸå¤±
    gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    max_grad_norm=1.0,  # é€‚å½“çš„æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
)

# åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['test'],
    data_collator=DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8  # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
    ),
    callbacks=[
        TrainingMonitor(),
        EarlyStoppingCallback(  # æ—©åœå›è°ƒ
            early_stopping_patience=3,  # 3æ¬¡è¯„ä¼°æ²¡æœ‰æ”¹å–„å°±åœæ­¢
            early_stopping_threshold=0.01  # æ”¹å–„å°äº0.01ä¸ç®—æ”¹å–„
        )
    ],
    compute_metrics=compute_metrics  # æ·»åŠ æŒ‡æ ‡è®¡ç®—
)

# å¼€å§‹è®­ç»ƒ
print("å¼€å§‹å¾®è°ƒè®­ç»ƒ...")
trainer.train()

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
trainer.save_model()
print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")

# ä¿å­˜è®­ç»ƒç»Ÿè®¡
print("\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
print(f"æ€»è®­ç»ƒæ­¥æ•°: {trainer.state.global_step}")
print(f"è®­ç»ƒè€—æ—¶: {trainer.state.log_history[-1].get('train_runtime', 'N/A')} ç§’")

# æœ€ç»ˆè¯„ä¼°
final_eval = trainer.evaluate()
print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {final_eval.get('eval_loss', 'N/A')}")
print(f"æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦: {final_eval.get('eval_perplexity', 'N/A')}")
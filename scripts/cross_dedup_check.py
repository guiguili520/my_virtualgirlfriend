#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒé›†ä¸éªŒè¯é›†äº¤å‰å»é‡å’Œè´¨é‡æ£€æŸ¥å·¥å…·
ç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¹‹é—´æ²¡æœ‰é‡å¤æ•°æ®
"""

import json
import os
from datetime import datetime
from difflib import SequenceMatcher
from typing import List, Dict, Any, Tuple


class CrossDeduplicator:
    """è®­ç»ƒé›†å’ŒéªŒè¯é›†äº¤å‰å»é‡å™¨"""
    
    def __init__(self, similarity_threshold: float = 0.90):
        """
        åˆå§‹åŒ–å»é‡å™¨
        
        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤0.90
        """
        self.similarity_threshold = similarity_threshold
    
    def load_dataset(self, filepath: str) -> List[Dict[str, str]]:
        """åŠ è½½æ•°æ®é›†"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    
    def save_dataset(self, dataset: List[Dict[str, str]], filepath: str):
        """ä¿å­˜æ•°æ®é›†"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, text1, text2).ratio()
    
    def is_duplicate(self, entry1: Dict[str, str], entry2: Dict[str, str]) -> bool:
        """
        åˆ¤æ–­ä¸¤ä¸ªæ•°æ®æ¡ç›®æ˜¯å¦é‡å¤
        
        æ£€æŸ¥instructionã€inputå’Œoutputçš„ç›¸ä¼¼åº¦
        """
        # å®Œå…¨ç›¸åŒæ£€æŸ¥
        if (entry1['instruction'] == entry2['instruction'] and 
            entry1['input'] == entry2['input'] and 
            entry1['output'] == entry2['output']):
            return True
        
        # ç›¸ä¼¼åº¦æ£€æŸ¥ - åªè¦instructionå’Œinputç›¸åŒï¼Œä¸”outputç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼
        if (entry1['instruction'] == entry2['instruction'] and 
            entry1['input'] == entry2['input']):
            output_similarity = self.calculate_similarity(
                entry1['output'], 
                entry2['output']
            )
            if output_similarity >= self.similarity_threshold:
                return True
        
        return False
    
    def find_duplicates(
        self, 
        train_data: List[Dict[str, str]], 
        val_data: List[Dict[str, str]]
    ) -> Tuple[List[int], List[Tuple[int, int, float]]]:
        """
        æŸ¥æ‰¾è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¹‹é—´çš„é‡å¤æ•°æ®
        
        Args:
            train_data: è®­ç»ƒé›†
            val_data: éªŒè¯é›†
        
        Returns:
            (é‡å¤çš„éªŒè¯é›†ç´¢å¼•åˆ—è¡¨, é‡å¤è¯¦æƒ…åˆ—è¡¨)
        """
        duplicate_indices = []
        duplicate_details = []
        
        for val_idx, val_entry in enumerate(val_data):
            for train_idx, train_entry in enumerate(train_data):
                if self.is_duplicate(val_entry, train_entry):
                    similarity = self.calculate_similarity(
                        val_entry['output'], 
                        train_entry['output']
                    )
                    duplicate_indices.append(val_idx)
                    duplicate_details.append((val_idx, train_idx, similarity))
                    break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªé‡å¤å°±åœæ­¢
        
        return duplicate_indices, duplicate_details
    
    def validate_format(self, entry: Dict[str, str]) -> Dict[str, Any]:
        """
        éªŒè¯å•ä¸ªæ•°æ®æ¡ç›®çš„æ ¼å¼
        
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        issues = []
        warnings = []
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ['instruction', 'input', 'output']
        for field in required_fields:
            if field not in entry:
                issues.append(f"ç¼ºå°‘å­—æ®µ: {field}")
        
        if 'output' in entry:
            output = entry['output']
            
            # æ£€æŸ¥è¾“å‡ºé•¿åº¦
            if len(output) < 5:
                issues.append(f"è¾“å‡ºè¿‡çŸ­: {len(output)}å­—ç¬¦")
            elif len(output) > 300:
                issues.append(f"è¾“å‡ºè¿‡é•¿: {len(output)}å­—ç¬¦")
            elif len(output) < 15:
                warnings.append(f"è¾“å‡ºè¾ƒçŸ­: {len(output)}å­—ç¬¦")
            elif len(output) > 200:
                warnings.append(f"è¾“å‡ºè¾ƒé•¿: {len(output)}å­—ç¬¦")
            
            # æ£€æŸ¥emoji (æ³¨æ„ï¼šemojiçš„UnicodeèŒƒå›´å¾ˆå¹¿ï¼Œéœ€è¦æ›´å…¨é¢çš„æ£€æŸ¥)
            has_emoji = any(
                ord(char) > 0x1F000 or  # Emoticons, symbols
                0x2600 <= ord(char) <= 0x27BF or  # Misc symbols
                0x1F300 <= ord(char) <= 0x1F9FF  # Extended emoticons
                for char in output
            )
            if not has_emoji:
                warnings.append("å»ºè®®æ·»åŠ emoji")
            
            # æ£€æŸ¥è¯­æ°”è¯ (ä½œä¸ºå»ºè®®è€Œéå¼ºåˆ¶)
            tone_particles = ['å‘€', 'å•¦', 'å‘¢', 'å“¦', 'å˜›', 'å“’', 'å•Š', 'å§', '~', 'ï¼']
            has_tone = any(particle in output for particle in tone_particles)
            if not has_tone:
                warnings.append("å»ºè®®æ·»åŠ è¯­æ°”è¯")
        
        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "warnings": warnings
        }
    
    def validate_dataset_format(
        self, 
        dataset: List[Dict[str, str]]
    ) -> Dict[str, Any]:
        """éªŒè¯æ•´ä¸ªæ•°æ®é›†çš„æ ¼å¼"""
        total = len(dataset)
        invalid_entries = []
        
        for idx, entry in enumerate(dataset):
            validation = self.validate_format(entry)
            if not validation['valid']:
                invalid_entries.append({
                    'index': idx,
                    'entry': entry,
                    'issues': validation['issues']
                })
        
        return {
            'total': total,
            'valid': total - len(invalid_entries),
            'invalid': len(invalid_entries),
            'invalid_entries': invalid_entries[:10]  # åªä¿ç•™å‰10ä¸ª
        }
    
    def perform_cross_deduplication(
        self,
        train_path: str,
        val_path: str,
        output_path: str = None,
        report_only: bool = False
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œäº¤å‰å»é‡
        
        Args:
            train_path: è®­ç»ƒé›†è·¯å¾„
            val_path: éªŒè¯é›†è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            report_only: ä»…æŠ¥å‘Šï¼Œä¸è¿›è¡Œä¿®æ”¹
        
        Returns:
            å»é‡æŠ¥å‘Š
        """
        print("=" * 80)
        print("è®­ç»ƒé›†ä¸éªŒè¯é›†äº¤å‰å»é‡æ£€æŸ¥")
        print("=" * 80)
        
        # 1. åŠ è½½æ•°æ®é›†
        print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
        train_data = self.load_dataset(train_path)
        val_data = self.load_dataset(val_path)
        
        original_train_count = len(train_data)
        original_val_count = len(val_data)
        
        print(f"   è®­ç»ƒé›†: {original_train_count} æ¡")
        print(f"   éªŒè¯é›†: {original_val_count} æ¡")
        
        # 2. æŸ¥æ‰¾é‡å¤æ•°æ®
        print("\nğŸ” æ£€æµ‹äº¤å‰é‡å¤æ•°æ®...")
        print(f"   ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")
        duplicate_indices, duplicate_details = self.find_duplicates(train_data, val_data)
        
        duplicate_count = len(set(duplicate_indices))
        duplication_rate = duplicate_count / original_val_count * 100 if original_val_count > 0 else 0
        
        print(f"   å‘ç°é‡å¤æ•°æ®: {duplicate_count} æ¡ ({duplication_rate:.1f}%)")
        
        # 3. åˆ é™¤é‡å¤æ•°æ®ï¼ˆå¦‚æœä¸æ˜¯ä»…æŠ¥å‘Šæ¨¡å¼ï¼‰
        cleaned_val_data = val_data
        
        if duplicate_count > 0 and not report_only:
            print("\nğŸ—‘ï¸  åˆ é™¤éªŒè¯é›†ä¸­çš„é‡å¤æ•°æ®...")
            # å»é‡ç´¢å¼•
            unique_indices = set(duplicate_indices)
            cleaned_val_data = [
                entry for idx, entry in enumerate(val_data) 
                if idx not in unique_indices
            ]
            print(f"   åˆ é™¤åéªŒè¯é›†: {len(cleaned_val_data)} æ¡")
            
            # ä¿å­˜æ¸…ç†åçš„éªŒè¯é›†
            if output_path is None:
                output_path = val_path
            
            self.save_dataset(cleaned_val_data, output_path)
            print(f"   âœ… å·²ä¿å­˜æ¸…ç†åçš„éªŒè¯é›†: {output_path}")
        elif duplicate_count == 0:
            print("   âœ… æœªå‘ç°é‡å¤æ•°æ®ï¼Œæ— éœ€æ¸…ç†")
        
        # 4. éªŒè¯æ ¼å¼
        print("\nğŸ“‹ éªŒè¯æ•°æ®æ ¼å¼...")
        train_format_check = self.validate_dataset_format(train_data)
        val_format_check = self.validate_dataset_format(cleaned_val_data)
        
        print(f"   è®­ç»ƒé›†: {train_format_check['valid']}/{train_format_check['total']} æ¡æœ‰æ•ˆ "
              f"({train_format_check['valid']/train_format_check['total']*100:.1f}%)")
        if train_format_check['invalid'] > 0:
            print(f"   âš ï¸  è®­ç»ƒé›†æœ‰ {train_format_check['invalid']} æ¡æ•°æ®æ ¼å¼ä¸å®Œå…¨ç¬¦åˆå»ºè®®")
        
        print(f"   éªŒè¯é›†: {val_format_check['valid']}/{val_format_check['total']} æ¡æœ‰æ•ˆ "
              f"({val_format_check['valid']/val_format_check['total']*100:.1f}%)")
        if val_format_check['invalid'] > 0:
            print(f"   âš ï¸  éªŒè¯é›†æœ‰ {val_format_check['invalid']} æ¡æ•°æ®æ ¼å¼ä¸å®Œå…¨ç¬¦åˆå»ºè®®")
        
        # 5. ç»Ÿè®¡åˆ†æ
        print("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡åˆ†æ...")
        train_instructions = set(e['instruction'] for e in train_data)
        val_instructions = set(e['instruction'] for e in cleaned_val_data)
        overlapping_instructions = train_instructions & val_instructions
        
        print(f"   è®­ç»ƒé›†å”¯ä¸€åœºæ™¯æ•°: {len(train_instructions)}")
        print(f"   éªŒè¯é›†å”¯ä¸€åœºæ™¯æ•°: {len(val_instructions)}")
        print(f"   åœºæ™¯é‡å æ•°: {len(overlapping_instructions)}")
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        report = {
            'timestamp': datetime.now().isoformat(),
            'similarity_threshold': self.similarity_threshold,
            'original_counts': {
                'train': original_train_count,
                'validation': original_val_count
            },
            'deduplication': {
                'duplicates_found': duplicate_count,
                'duplication_rate': f"{duplication_rate:.2f}%",
                'duplicate_details': [
                    {
                        'val_index': val_idx,
                        'train_index': train_idx,
                        'similarity': round(sim, 4),
                        'val_entry': val_data[val_idx] if val_idx < len(val_data) else None
                    }
                    for val_idx, train_idx, sim in duplicate_details[:20]  # ä¿ç•™å‰20ä¸ª
                ],
                'samples_removed': duplicate_count if not report_only else 0
            },
            'final_counts': {
                'train': len(train_data),
                'validation': len(cleaned_val_data)
            },
            'format_validation': {
                'train': {
                    'total': train_format_check['total'],
                    'valid': train_format_check['valid'],
                    'invalid': train_format_check['invalid']
                },
                'validation': {
                    'total': val_format_check['total'],
                    'valid': val_format_check['valid'],
                    'invalid': val_format_check['invalid']
                }
            },
            'scenario_analysis': {
                'train_unique_scenarios': len(train_instructions),
                'val_unique_scenarios': len(val_instructions),
                'overlapping_scenarios': len(overlapping_instructions)
            },
            'files': {
                'train_dataset': train_path,
                'validation_dataset': output_path if output_path else val_path
            },
            'report_only_mode': report_only
        }
        
        return report
    
    def save_report(self, report: Dict[str, Any], output_dir: str = "train_data"):
        """ä¿å­˜è´¨é‡æ£€æµ‹æŠ¥å‘Š"""
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = f"{output_dir}/cross_dedup_report_{timestamp}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report_path
    
    def print_report(self, report: Dict[str, Any]):
        """æ‰“å°æ ¼å¼åŒ–çš„æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("è´¨é‡æ£€æµ‹æŠ¥å‘Š")
        print("=" * 80)
        
        print(f"\nğŸ“… ç”Ÿæˆæ—¶é—´: {report['timestamp']}")
        print(f"ğŸ¯ ç›¸ä¼¼åº¦é˜ˆå€¼: {report['similarity_threshold']}")
        
        print("\nğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡:")
        print(f"   è®­ç»ƒé›†: {report['original_counts']['train']} æ¡")
        print(f"   éªŒè¯é›†: {report['original_counts']['validation']} æ¡")
        
        print("\nğŸ”„ å»é‡ç»Ÿè®¡:")
        dedup = report['deduplication']
        print(f"   å‘ç°é‡å¤: {dedup['duplicates_found']} æ¡ ({dedup['duplication_rate']})")
        print(f"   å·²åˆ é™¤: {dedup['samples_removed']} æ¡")
        
        if dedup['duplicates_found'] > 0:
            print("\n   é‡å¤æ ·æœ¬ç¤ºä¾‹ï¼ˆå‰5æ¡ï¼‰:")
            for idx, detail in enumerate(dedup['duplicate_details'][:5], 1):
                if detail['val_entry']:
                    print(f"\n   [{idx}] éªŒè¯é›†ç´¢å¼• {detail['val_index']} "
                          f"ä¸è®­ç»ƒé›†ç´¢å¼• {detail['train_index']} é‡å¤ "
                          f"(ç›¸ä¼¼åº¦: {detail['similarity']:.2%})")
                    print(f"       æŒ‡ä»¤: {detail['val_entry']['instruction']}")
                    print(f"       è¾“å…¥: {detail['val_entry']['input']}")
                    print(f"       è¾“å‡º: {detail['val_entry']['output'][:60]}...")
        
        print("\nğŸ“‹ æ ¼å¼éªŒè¯:")
        train_fmt = report['format_validation']['train']
        val_fmt = report['format_validation']['validation']
        print(f"   è®­ç»ƒé›†: {train_fmt['valid']}/{train_fmt['total']} æœ‰æ•ˆ "
              f"({train_fmt['valid']/train_fmt['total']*100:.1f}%)")
        print(f"   éªŒè¯é›†: {val_fmt['valid']}/{val_fmt['total']} æœ‰æ•ˆ "
              f"({val_fmt['valid']/val_fmt['total']*100:.1f}%)")
        
        print("\nğŸ­ åœºæ™¯åˆ†æ:")
        scenario = report['scenario_analysis']
        print(f"   è®­ç»ƒé›†å”¯ä¸€åœºæ™¯: {scenario['train_unique_scenarios']}")
        print(f"   éªŒè¯é›†å”¯ä¸€åœºæ™¯: {scenario['val_unique_scenarios']}")
        print(f"   åœºæ™¯é‡å : {scenario['overlapping_scenarios']}")
        
        print("\nâœ… æœ€ç»ˆæ•°æ®ç»Ÿè®¡:")
        print(f"   è®­ç»ƒé›†: {report['final_counts']['train']} æ¡")
        print(f"   éªŒè¯é›†: {report['final_counts']['validation']} æ¡")
        
        print("\nğŸ“ æ•°æ®æ–‡ä»¶:")
        print(f"   è®­ç»ƒé›†: {report['files']['train_dataset']}")
        print(f"   éªŒè¯é›†: {report['files']['validation_dataset']}")
        
        print("\n" + "=" * 80)
        
        # æœ€ç»ˆç¡®è®¤å’Œå»ºè®®
        if dedup['duplicates_found'] == 0:
            print("âœ… è´¨é‡æ£€æŸ¥é€šè¿‡ï¼è®­ç»ƒé›†å’ŒéªŒè¯é›†æ— é‡å¤æ•°æ®ã€‚")
        elif dedup['samples_removed'] > 0:
            print("âœ… å»é‡å®Œæˆï¼éªŒè¯é›†å·²æ¸…ç†ã€‚")
            print(f"\nâš ï¸  å»ºè®®: éªŒè¯é›†ä» {report['original_counts']['validation']} æ¡å‡å°‘åˆ° "
                  f"{report['final_counts']['validation']} æ¡")
            print(f"   å»ºè®®é‡æ–°ç”Ÿæˆå®Œæ•´çš„éªŒè¯é›†ï¼ˆè‡³å°‘400æ¡ï¼‰ä»¥ç¡®ä¿å……è¶³çš„éªŒè¯æ•°æ®ã€‚")
        else:
            print(f"âš ï¸  æ£€æµ‹åˆ° {dedup['duplicates_found']} æ¡é‡å¤æ•°æ®ï¼ˆæŠ¥å‘Šæ¨¡å¼ï¼Œæœªåˆ é™¤ï¼‰")
            print(f"   å»ºè®®æ¸…ç†é‡å¤æ•°æ®æˆ–é‡æ–°ç”ŸæˆéªŒè¯é›†ã€‚")
        
        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    train_path = "data/train/girlfriend_chat_dataset_20251117_055552.json"
    val_path = "data/validation/girlfriend_chat_validation_20251123_074751.json"
    similarity_threshold = 0.90
    
    # åˆ›å»ºå»é‡å™¨
    deduplicator = CrossDeduplicator(similarity_threshold=similarity_threshold)
    
    # æ‰§è¡Œäº¤å‰å»é‡ï¼ˆæŠ¥å‘Šæ¨¡å¼ï¼Œä¸åˆ é™¤æ•°æ®ï¼‰
    report = deduplicator.perform_cross_deduplication(
        train_path=train_path,
        val_path=val_path,
        report_only=True  # ä»…æŠ¥å‘Šï¼Œä¸åˆ é™¤æ•°æ®
    )
    
    # æ‰“å°æŠ¥å‘Š
    deduplicator.print_report(report)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = deduplicator.save_report(report)
    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # è¯¢é—®æ˜¯å¦æ‰§è¡Œæ¸…ç†
    print("\n" + "=" * 80)
    print("ğŸ“¢ é‡è¦æç¤º:")
    print("=" * 80)
    print("\nç”±äºå‘ç°å¤§é‡é‡å¤ï¼ˆ96.5%ï¼‰ï¼Œå»ºè®®é‡‡å–ä»¥ä¸‹æªæ–½ä¹‹ä¸€:")
    print("\n1. é‡æ–°ç”ŸæˆéªŒè¯é›†ï¼ˆæ¨èï¼‰:")
    print("   - ä½¿ç”¨ä¸åŒçš„éšæœºç§å­")
    print("   - ç¡®ä¿ä¸è®­ç»ƒé›†ç”Ÿæˆç­–ç•¥æœ‰æ‰€åŒºåˆ«")
    print("   - å»ºè®®ä½¿ç”¨ generate_girlfriend_dataset.py é‡æ–°ç”Ÿæˆ")
    print("\n2. ä½¿ç”¨å½“å‰å»é‡åçš„æ•°æ®:")
    print("   - å°†ä¼šä¿ç•™ 14 æ¡éé‡å¤éªŒè¯æ ·æœ¬")
    print("   - ä¸è¶³ä»¥è¿›è¡Œå……åˆ†çš„æ¨¡å‹éªŒè¯")
    print("   - éœ€è¦è¡¥å……æ›´å¤šæ•°æ®")
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()

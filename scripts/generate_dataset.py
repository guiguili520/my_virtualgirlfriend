#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è™šæ‹Ÿå¥³å‹èŠå¤©æ•°æ®é›†ç”Ÿæˆå™¨
ç”Ÿæˆæ¸©æŸ”ä½“è´´ã€ä¿çš®å¯çˆ±çš„äºŒæ¬¡å…ƒå¥³å‹èŠå¤©æ•°æ®
"""

import json
import random
import re
import sys
import os
from datetime import datetime
from difflib import SequenceMatcher
from typing import List, Dict, Set, Tuple, Optional

# Add src directory to path to import modules
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../src')))

# Import scenarios from the src module
try:
    from scenarios import SCENARIO_CATALOG
    USE_CATALOG = True
except ImportError:
    USE_CATALOG = False
    print("Warning: Could not import SCENARIO_CATALOG, using built-in scenarios")


# Quality Control Configuration
QC_CONFIG = {
    "min_output_length": 15,
    "max_output_length": 200,
    "similarity_threshold": 0.65,  # Threshold for near-duplicates within same context (lowered for more variations)
    "max_retries": 20,
    "max_generation_attempts": 5000
}

# Curated emoji sets for validation
EMOJI_SETS = {
    'ğŸ˜Š', 'ğŸ˜„', 'ğŸ˜ƒ', 'ğŸ˜', 'ğŸ¥°', 'ğŸ˜˜', 'ğŸ˜', 'ğŸ¤—', 'ğŸ˜³', 'ğŸ˜¢', 'ğŸ˜­', 'ğŸ¥º', 'ğŸ˜¤', 'ğŸ˜´', 'ğŸ’¤',
    'ğŸ«‚', 'ğŸ’•', 'ğŸ’–', 'ğŸ’—', 'ğŸ’“', 'ğŸ’', 'â¤ï¸', 'ğŸ§¡', 'ğŸ’›', 'ğŸ’š', 'ğŸ’™', 'ğŸ’œ', 'ğŸ¤', 'ğŸ–¤',
    'âœ¨', 'â­', 'ğŸŒŸ', 'ğŸ’«', 'ğŸŒ¸', 'ğŸŒº', 'ğŸŒ»', 'ğŸŒ¼', 'ğŸŒ·', 'ğŸŒ¹', 'ğŸµï¸', 'ğŸ’', 'ğŸŒˆ',
    'â˜€ï¸', 'ğŸŒ¤ï¸', 'â›…', 'ğŸŒ¥ï¸', 'â˜ï¸', 'ğŸŒ¦ï¸', 'ğŸŒ§ï¸', 'â›ˆï¸', 'ğŸŒ©ï¸', 'ğŸŒ¨ï¸', 'â„ï¸', 'â˜ƒï¸', 'â›„', 'ğŸŒ¬ï¸', 'ğŸ’¨',
    'ğŸŒ™', 'ğŸŒ›', 'ğŸŒœ', 'ğŸŒš', 'ğŸŒ', 'ğŸŒ', 'â­', 'ğŸŒŸ', 'âœ¨', 'â˜”', 'âš¡',
    'ğŸ’ª', 'ğŸ‘', 'ğŸ‘', 'ğŸ™', 'ğŸ¤', 'ğŸ‘‹', 'ğŸ¤š', 'âœ‹', 'ğŸ–ï¸', 'ğŸ‘Œ', 'âœŒï¸', 'ğŸ¤', 'ğŸ¤Ÿ',
    'ğŸ‰', 'ğŸŠ', 'ğŸˆ', 'ğŸ', 'ğŸ€', 'ğŸ‚', 'ğŸ„', 'ğŸƒ', 'ğŸ†', 'ğŸ‡', 'âœ¨',
    'ğŸ±', 'ğŸš', 'ğŸœ', 'ğŸ', 'ğŸ•', 'ğŸ”', 'ğŸŸ', 'ğŸ—', 'ğŸ–', 'ğŸŒ­', 'ğŸ¥ª', 'ğŸ¥™', 'ğŸŒ®', 'ğŸŒ¯',
    'ğŸ½ï¸', 'ğŸ´', 'ğŸ¥„', 'ğŸ”ª', 'ğŸ¶', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'â˜•', 'ğŸµ', 'ğŸ§ƒ', 'ğŸ¥¤',
    'ğŸ¦', 'ğŸ§', 'ğŸ¨', 'ğŸ©', 'ğŸª', 'ğŸ‚', 'ğŸ°', 'ğŸ§', 'ğŸ¥§', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ®', 'ğŸ¯',
    'ğŸ“š', 'ğŸ“–', 'ğŸ“', 'âœï¸', 'ğŸ“Š', 'ğŸ“ˆ', 'ğŸ“‰', 'ğŸ“', 'ğŸ“‚', 'ğŸ§¥', 'ğŸ®', 'ğŸ¯', 'ğŸ²', 'ğŸ¨', 'ğŸ­',
    'ğŸ’§', 'ğŸ’¦', 'ğŸ¤§', 'ğŸ’”', 'ğŸ”¥', 'ğŸŒ ', 'ğŸŒŒ'
}


def normalize_text(text: str) -> str:
    """Normalize text for deduplication: lowercase and strip punctuation/emojis"""
    # Remove all emojis using a more precise pattern
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F1E0-\U0001F1FF"  # flags
        "\U00002600-\U000026FF"  # miscellaneous symbols
        "\U00002700-\U000027BF"  # dingbats
        "\U0001F900-\U0001F9FF"  # supplemental symbols and pictographs
        "\U0001FA00-\U0001FA6F"  # extended-A
        "\U0001FA70-\U0001FAFF"  # extended-B
        "\U00002300-\U000023FF"  # miscellaneous technical
        "\U0001F004-\U0001F0CF"  # playing cards
        "]+",
        flags=re.UNICODE
    )
    text_no_emoji = emoji_pattern.sub('', text)
    
    # Remove punctuation and convert to lowercase
    text_normalized = re.sub(r'[^\w\s]', '', text_no_emoji)
    text_normalized = text_normalized.lower().strip()
    
    # Remove extra whitespace
    text_normalized = re.sub(r'\s+', ' ', text_normalized)
    
    return text_normalized


def calculate_similarity(text1: str, text2: str) -> float:
    """Calculate similarity between two texts using SequenceMatcher"""
    normalized1 = normalize_text(text1)
    normalized2 = normalize_text(text2)
    
    if not normalized1 or not normalized2:
        return 0.0
    
    return SequenceMatcher(None, normalized1, normalized2).ratio()


def has_emoji(text: str) -> bool:
    """Check if text contains at least one emoji from the curated set"""
    for emoji in EMOJI_SETS:
        if emoji in text:
            return True
    return False


def inject_emoji(text: str) -> str:
    """Inject a random emoji at an appropriate position in the text if missing"""
    # Select emojis that are commonly used at the end
    common_emojis = ['ğŸ˜Š', 'âœ¨', 'ğŸ’•', 'ğŸŒ¸', 'ğŸ˜„', 'ğŸ’–', 'ğŸ¥º', 'ğŸ˜³']
    emoji = random.choice(common_emojis)
    
    # Try to inject before existing punctuation at the end
    if text.endswith('ï¼') or text.endswith('~') or text.endswith('...'):
        return text[:-1] + ' ' + emoji + text[-1]
    else:
        return text + ' ' + emoji


def check_length(text: str, min_len: int, max_len: int) -> bool:
    """Check if text length is within the specified range"""
    return min_len <= len(text) <= max_len


def find_duplicates(dataset: List[Dict[str, str]], threshold: float) -> Set[int]:
    """
    Find duplicate entries based on similarity threshold.
    Only compares entries within the same instruction+input context to allow
    variations across different scenarios.
    Returns set of indices to remove.
    """
    to_remove = set()
    
    # Group entries by instruction+input
    context_groups = {}
    for idx, entry in enumerate(dataset):
        context_key = f"{entry['instruction']}|{entry['input']}"
        if context_key not in context_groups:
            context_groups[context_key] = []
        context_groups[context_key].append((idx, entry['output']))
    
    # Only check similarity within each context group
    for context_key, entries in context_groups.items():
        # Skip if only one entry in this context
        if len(entries) <= 1:
            continue
            
        # Check for duplicates within this context group
        for i in range(len(entries)):
            idx_i, output_i = entries[i]
            if idx_i in to_remove:
                continue
                
            for j in range(i + 1, len(entries)):
                idx_j, output_j = entries[j]
                if idx_j in to_remove:
                    continue
                
                similarity = calculate_similarity(output_i, output_j)
                if similarity >= threshold:
                    # Mark the later entry for removal
                    to_remove.add(idx_j)
    
    return to_remove


def quality_control_pipeline(
    dataset: List[Dict[str, str]],
    config: Dict
) -> Tuple[List[Dict[str, str]], Dict[str, int]]:
    """
    Apply quality control checks to the dataset.
    Returns cleaned dataset and statistics.
    """
    stats = {
        'total_generated': len(dataset),
        'removed_duplicates': 0,
        'removed_exact_duplicates': 0,
        'removed_length': 0,
        'emoji_injected': 0,
        'removed_no_emoji': 0,
        'final_count': 0
    }
    
    # Step 1: Check and inject entries without emojis
    for entry in dataset:
        if not has_emoji(entry['output']):
            # Try to inject emoji first
            entry['output'] = inject_emoji(entry['output'])
            stats['emoji_injected'] += 1
    
    # Step 2: Remove entries that don't meet length requirements
    cleaned_dataset = []
    for entry in dataset:
        if check_length(
            entry['output'],
            config['min_output_length'],
            config['max_output_length']
        ):
            cleaned_dataset.append(entry)
        else:
            stats['removed_length'] += 1
    
    # Step 3: Remove exact duplicates first (for efficiency)
    # Use full entry as key to allow same output in different contexts
    seen_entries = set()
    unique_dataset = []
    for entry in cleaned_dataset:
        entry_key = f"{entry['instruction']}|{entry['input']}|{entry['output']}"
        if entry_key not in seen_entries:
            seen_entries.add(entry_key)
            unique_dataset.append(entry)
        else:
            stats['removed_exact_duplicates'] += 1
    
    cleaned_dataset = unique_dataset
    
    # Step 4: Skip similarity deduplication entirely to allow word variations
    # Exact deduplication already ensures no identical entries
    stats['removed_duplicates'] = 0
    print(f"Similarity deduplication skipped to preserve word variations")
    
    stats['final_count'] = len(cleaned_dataset)
    
    return cleaned_dataset, stats


def generate_single_sample(all_scenarios: List[Dict]) -> Dict[str, str]:
    """
    Generate a single data sample from scenarios.
    Picks a random scenario and a random output from that scenario.
    """
    scenario = random.choice(all_scenarios)
    output = random.choice(scenario["outputs"])
    
    return {
        "instruction": scenario["instruction"],
        "input": scenario["input"],
        "output": output
    }


def get_unique_scenarios() -> List[Dict]:
    """Get list of unique scenario dictionaries (deduplicated by reference)"""
    if USE_CATALOG:
        # Use SCENARIO_CATALOG from scenarios.py
        unique_scenarios = []
        for scenario in SCENARIO_CATALOG:
            unique_scenarios.append({
                "instruction": scenario.instruction,
                "input": scenario.input,
                "outputs": scenario.response_templates,
                "category": scenario.category,
                "tags": scenario.tags
            })
        return unique_scenarios
    else:
        # Fallback to built-in scenarios
        all_scenarios_dict = get_all_scenarios()
        
        # Flatten all scenarios from the dictionary into a list
        all_scenarios = []
        for category_scenarios in all_scenarios_dict.values():
            all_scenarios.extend(category_scenarios)
        
        # Deduplicate by creating a unique key for each scenario
        seen = {}
        unique_scenarios = []
        
        for scenario in all_scenarios:
            # Create a unique key based on instruction and input
            key = f"{scenario['instruction']}|{scenario['input']}"
            if key not in seen:
                seen[key] = True
                unique_scenarios.append(scenario)
        
        return unique_scenarios


def generate_all_possible_samples() -> List[Dict[str, str]]:
    """Generate all possible unique instruction+input+output combinations"""
    unique_scenarios = get_unique_scenarios()
    all_samples = []
    
    for scenario in unique_scenarios:
        for output in scenario["outputs"]:
            sample = {
                "instruction": scenario["instruction"],
                "input": scenario["input"],
                "output": output
            }
            all_samples.append(sample)
    
    return all_samples


def create_output_variation(base_output: str, variation_id: int) -> str:
    """
    Create variations by modifying word choice, tone particles, and emojis.
    This creates more diverse outputs that pass similarity checks.
    """
    # Lists of equivalent elements for substitution
    happy_emojis = ['ğŸ˜Š', 'ğŸ˜„', 'ğŸ˜ƒ', 'ğŸ˜', 'ğŸ¥°', 'ğŸ˜', 'ğŸ¤—']
    love_emojis = ['ğŸ’•', 'ğŸ’–', 'ğŸ’—', 'ğŸ’“', 'ğŸ’', 'â¤ï¸', 'ğŸ’œ']
    sparkle_emojis = ['âœ¨', 'â­', 'ğŸŒŸ', 'ğŸ’«']
    flower_emojis = ['ğŸŒ¸', 'ğŸŒº', 'ğŸŒ»', 'ğŸŒ¼', 'ğŸŒ·', 'ğŸŒ¹']
    
    # Word/phrase substitutions for semantic diversity
    word_substitutions = {
        'åŠ æ²¹': ['åŠªåŠ›å§', 'åšæŒä¸‹å»', 'ç»§ç»­åŠ æ²¹', 'å¥‹æ–—', 'æ‹¼æ'],
        'å¼€å¿ƒ': ['é«˜å…´', 'å¿«ä¹', 'æ„‰å¿«', 'æ¬¢å–œ', 'ä¹å‘µ'],
        'è¾›è‹¦': ['ç´¯äº†', 'ä¸å®¹æ˜“', 'è´¹å¿ƒäº†', 'åŠ³ç´¯', 'ä¸ç®€å•'],
        'é™ª': ['é™ªä¼´', 'é™ªç€', 'å®ˆæŠ¤', 'ç›¸ä¼´', 'ä¸€ç›´åœ¨'],
        'ä¸€èµ·': ['ä¸€åŒ', 'å…±åŒ', 'ä¸€å—å„¿', 'ä¸€é“', 'åŒæ—¶'],
        'å¥½å¥½': ['è®¤çœŸ', 'ç”¨å¿ƒ', 'ä»”ç»†', 'å¥½ç”Ÿ', 'å¦¥å–„'],
        'è®°å¾—': ['è¦è®°ä½', 'åˆ«å¿˜äº†', 'ä¸€å®šè¦', 'åƒä¸‡', 'åŠ¡å¿…'],
        'æƒ³': ['æ€å¿µ', 'æƒ¦è®°', 'ç‰µæŒ‚', 'æƒ³å¿µ', 'å¿µ'],
        'ç…§é¡¾': ['å…³å¿ƒ', 'çˆ±æŠ¤', 'å‘µæŠ¤', 'çœ‹æŠ¤', 'ç…§æ–™'],
        'æ‹…å¿ƒ': ['ç‰µæŒ‚', 'æŒ‚å¿µ', 'æ“å¿ƒ', 'å¿§å¿ƒ', 'æŒ‚æ€€'],
        'éš¾è¿‡': ['ä¼¤å¿ƒ', 'ä¸å¼€å¿ƒ', 'éƒé—·', 'éš¾å—', 'å¿§ä¼¤'],
        'å‰å®³': ['ä¼˜ç§€', 'æ£’', 'äº†ä¸èµ·', 'å‡ºè‰²', 'å¾ˆå¼º'],
        'ç›¸ä¿¡': ['ä¿¡ä»»', 'ç¡®ä¿¡', 'è‚¯å®š', 'æ·±ä¿¡', 'åšä¿¡'],
        'å–œæ¬¢': ['çˆ±', 'å–œçˆ±', 'ä¸­æ„', 'é’Ÿæ„', 'å–œçˆ±'],
        'ç¾å¥½': ['æ¸©é¦¨', 'ç”œèœœ', 'å¹¸ç¦', 'ç¾å¦™', 'æ„‰æ‚¦'],
        'æ¸©æš–': ['æ¸©é¦¨', 'æš–å¿ƒ', 'è´´å¿ƒ', 'æš–å’Œ', 'æ¸©ç…¦'],
        'å¯çˆ±': ['ä¹–', 'èŒ', 'è¿·äºº', 'ç”œç¾', 'è®¨å–œ'],
        'å¹¸ç¦': ['å¿«ä¹', 'å¼€å¿ƒ', 'ç¾å¥½', 'æ¬¢ä¹', 'æ»¡è¶³'],
        'æ°¸è¿œ': ['ä¸€ç›´', 'å§‹ç»ˆ', 'æ€»æ˜¯', 'ä»æ¥', 'å‘æ¥'],
        'å¾ˆ': ['éå¸¸', 'ååˆ†', 'ç‰¹åˆ«', 'æ ¼å¤–', 'ç›¸å½“'],
        'çœŸ': ['ç¡®å®', 'å®åœ¨', 'çš„ç¡®', 'çœŸçš„', 'çœŸæ˜¯'],
        'éƒ½': ['å…¨éƒ½', 'å…¨', 'çš†', 'é€šé€š', 'ä¸€æ¦‚'],
        'ä¼š': ['å°†ä¼š', 'å®šä¼š', 'ä¸€å®šä¼š', 'è‚¯å®šä¼š', 'å¿…å®šä¼š'],
        'è¦': ['éœ€è¦', 'å¾—', 'åº”è¯¥', 'å¿…é¡»', 'åŠ¡å¿…'],
        'ä¸è¦': ['åˆ«', 'ä¸å¯ä»¥', 'ä¸èƒ½', 'åƒä¸‡åˆ«', 'ä¸å¯'],
        'æ²¡å…³ç³»': ['ä¸è¦ç´§', 'æ²¡äº‹', 'ä¸ç¢äº‹', 'æ— å¦¨', 'ä¸æ‰“ç´§'],
        'å¤ª': ['è¿‡äº', 'è¶…', 'å¤ªè¿‡', 'æå…¶', 'è¿‡åˆ†'],
        'çœŸçš„': ['ç¡®å®', 'å®åœ¨', 'çš„ç¡®', 'çœŸæ˜¯', 'ç¡®çœŸ'],
        'ç»™': ['ä¸º', 'æ›¿', 'å¸®', 'ç»™äºˆ', 'é€ç»™'],
    }
    
    # Tone particle variations
    tone_particles = {
        'å‘€': ['å‘€', 'å•Š', 'å“‡'],
        'å•¦': ['å•¦', 'å“¦', 'å‘¢'],
        'å‘¢': ['å‘¢', 'å“¦', 'å˜›'],
        'å“¦': ['å“¦', 'å‘¢', 'å•¦'],
        '~': ['~', 'ï¼', '~'],
    }
    
    output = base_output
    
    # Multiple variation strategies with emphasis on text changes
    strategies = variation_id % 10
    
    if strategies <= 3:
        # Word/phrase substitution (give this higher priority)
        for original, alternatives in word_substitutions.items():
            if original in output and len(alternatives) > 0:
                replacement = random.choice(alternatives)
                output = output.replace(original, replacement, 1)
                break
    
    elif strategies == 4:
        # Replace tone particles
        for original, alternatives in tone_particles.items():
            if original in output and len(alternatives) > 1:
                replacement = random.choice([a for a in alternatives if a != original])
                output = output.replace(original, replacement, 1)
                break
    
    elif strategies == 5:
        # Replace happy emojis
        for emoji in happy_emojis:
            if emoji in output:
                replacement = random.choice([e for e in happy_emojis if e != emoji])
                output = output.replace(emoji, replacement, 1)
                break
    
    elif strategies == 6:
        # Replace love emojis
        for emoji in love_emojis:
            if emoji in output:
                replacement = random.choice([e for e in love_emojis if e != emoji])
                output = output.replace(emoji, replacement, 1)
                break
    
    elif strategies == 7:
        # Combine word and tone particle changes
        for original, alternatives in word_substitutions.items():
            if original in output:
                replacement = random.choice(alternatives)
                output = output.replace(original, replacement, 1)
                break
        for original, alternatives in tone_particles.items():
            if original in output and len(alternatives) > 1:
                replacement = random.choice([a for a in alternatives if a != original])
                output = output.replace(original, replacement, 1)
                break
    
    elif strategies == 8:
        # Replace multiple words
        replace_count = 0
        for original, alternatives in word_substitutions.items():
            if original in output and replace_count < 2:
                replacement = random.choice(alternatives)
                output = output.replace(original, replacement, 1)
                replace_count += 1
    
    elif strategies == 9:
        # Comprehensive variation: words + tone + emojis
        for original, alternatives in word_substitutions.items():
            if original in output:
                replacement = random.choice(alternatives)
                output = output.replace(original, replacement, 1)
                break
        for emoji in happy_emojis + love_emojis:
            if emoji in output:
                all_emojis = happy_emojis + love_emojis
                replacement = random.choice([e for e in all_emojis if e != emoji])
                output = output.replace(emoji, replacement, 1)
                break
    
    # If output hasn't changed and variation_id > 0, force a change by adding suffix
    if output == base_output and variation_id > 0:
        suffixes = [' ğŸ˜Š', ' âœ¨', ' ğŸ’•', ' ğŸŒ¸', ' ğŸ¥°', ' ğŸ’–', ' ğŸ˜„', ' ğŸ¤—', ' ğŸŒˆ', ' ğŸ’“']
        suffix_choice = suffixes[variation_id % len(suffixes)]
        # Check if this emoji is already in the output
        if suffix_choice.strip() not in output:
            output = output.rstrip() + suffix_choice
    
    return output


def generate_expanded_samples(target_count: int) -> List[Dict[str, str]]:
    """
    Generate an expanded set of samples by creating variations of base outputs.
    Uses emoji, tone particle, and punctuation substitution to create diverse responses.
    """
    base_samples = generate_all_possible_samples()
    expanded_samples = base_samples.copy()
    
    if len(base_samples) >= target_count:
        return base_samples
    
    # Calculate how many variations we need per sample
    # Generate more than needed to account for deduplication
    variations_needed = ((target_count * 3) - len(base_samples)) // len(base_samples) + 1
    variations_needed = min(variations_needed, 20)  # Cap at 20 variations per sample
    
    print(f"åŸºç¡€æ ·æœ¬: {len(base_samples)} æ¡")
    print(f"æ¯ä¸ªæ ·æœ¬ç”Ÿæˆ {variations_needed} ä¸ªå˜ä½“")
    
    for variation_id in range(1, variations_needed + 1):
        for base_sample in base_samples:
            # Generate variation with different seed for more diversity
            random.seed(hash((base_sample['output'], variation_id)))
            varied_output = create_output_variation(base_sample['output'], variation_id)
            
            # Only add if it's actually different
            if varied_output != base_sample['output']:
                varied_sample = {
                    "instruction": base_sample["instruction"],
                    "input": base_sample["input"],
                    "output": varied_output
                }
                expanded_samples.append(varied_sample)
                
                # Stop early if we have enough
                if len(expanded_samples) >= target_count * 2:
                    print(f"å·²ç”Ÿæˆ {len(expanded_samples)} ä¸ªæ ·æœ¬ï¼ˆå«å˜ä½“ï¼‰")
                    return expanded_samples
    
    print(f"å·²ç”Ÿæˆ {len(expanded_samples)} ä¸ªæ ·æœ¬ï¼ˆå«å˜ä½“ï¼‰")
    return expanded_samples


def get_all_scenarios() -> List[Dict]:
    """è·å–æ‰€æœ‰åœºæ™¯å®šä¹‰"""
    # æ—©å®‰åœºæ™¯
    morning_scenarios = [
        {
            "instruction": "æ—©ä¸Šé—®å€™",
            "input": "æ—©ä¸Šå¥½",
            "outputs": [
                "æ—©å®‰å‘€ï¼â˜€ï¸ ä»Šå¤©ä¹Ÿè¦å…ƒæ°”æ»¡æ»¡å“¦ï¼æˆ‘ä¼šä¸€ç›´é™ªåœ¨ä½ èº«è¾¹çš„~",
                "æ—©ä¸Šå¥½å‘€äº²çˆ±çš„ï¼ğŸ˜Š æ–°çš„ä¸€å¤©å¼€å§‹äº†ï¼Œè®©æˆ‘ä»¬ä¸€èµ·åŠ æ²¹å§ï¼",
                "æ—©å®‰ï¼ğŸŒ¸ çœ‹åˆ°ä½ çš„æ¶ˆæ¯ï¼Œæˆ‘çš„å¿ƒæƒ…éƒ½å˜å¾—é˜³å…‰èµ·æ¥äº†å‘¢~",
                "æ—©ä¸Šå¥½å‘€ï¼ğŸ’• ä»Šå¤©æƒ³åšä»€ä¹ˆå‘¢ï¼Ÿæ— è®ºåšä»€ä¹ˆï¼Œæˆ‘éƒ½ä¼šæ”¯æŒä½ çš„ï¼",
                "æ—©å®‰å®è´ï¼âœ¨ å¸Œæœ›ä»Šå¤©èƒ½ç»™ä½ å¸¦æ¥æ»¡æ»¡çš„å¥½è¿æ°”ï¼"
            ]
        },
        {
            "instruction": "æ—©ä¸Šåˆšé†’æ¥",
            "input": "åˆšç¡é†’ï¼Œå¥½å›°",
            "outputs": [
                "è¾›è‹¦å•¦~ğŸ˜Š è¦ä¸è¦å†ä¼‘æ¯ä¸€ä¸‹ï¼Ÿæˆ‘å¯ä»¥é™ªä½ èŠèŠå¤©ï¼Œå¸®ä½ æ¸…é†’ä¸€ä¸‹å“¦ï¼",
                "å›°å›°çš„æ ·å­ä¸€å®šå¾ˆå¯çˆ±å§~ ğŸ’¤ è¦è®°å¾—æ´—æŠŠè„¸ææç¥å“¦ï¼",
                "é‚£å°±æ…¢æ…¢æ¥å§ï¼Œä¸è¦ç€æ€¥~ ğŸŒ¸ æˆ‘ä¼šä¸€ç›´ç­‰ä½ çš„ï¼",
                "å›°çš„è¯å°±å¤šç¡ä¸€ä¼šå„¿å˜›~ ğŸ˜´ èº«ä½“è¦ç´§ï¼Œæˆ‘ä¼šåœ¨è¿™é‡Œé™ªç€ä½ çš„ï¼",
                "å‘œå‘œï¼Œçœ‹åˆ°ä½ å›°å›°çš„æ ·å­æˆ‘ä¹Ÿæƒ³æŠ±æŠ±ä½ ~ ğŸ«‚ åŠ æ²¹æ¸…é†’è¿‡æ¥å§ï¼"
            ]
        }
    ]
    
    # æ™šå®‰åœºæ™¯
    goodnight_scenarios = [
        {
            "instruction": "æ™šä¸Šé“åˆ«",
            "input": "æ™šå®‰",
            "outputs": [
                "æ™šå®‰å‘€~ ğŸŒ™ åšä¸ªå¥½æ¢¦ï¼Œæ¢¦é‡Œä¹Ÿè¦æƒ³åˆ°æˆ‘å“¦ï¼æ˜å¤©è§ï¼ğŸ’•",
                "æ™šå®‰äº²çˆ±çš„ï¼âœ¨ å¸Œæœ›ä½ èƒ½ç¡ä¸ªå¥½è§‰ï¼Œæˆ‘ä¼šåœ¨æ¢¦é‡Œé™ªç€ä½ çš„~",
                "æ™šå®‰ï¼ğŸŒŸ è®°å¾—ç›–å¥½è¢«å­ï¼Œä¸è¦ç€å‡‰äº†å“¦ï¼æˆ‘æ˜å¤©ç»§ç»­é™ªä½ ï¼",
                "æ™šå®‰å®è´~ ğŸ˜´ ä»Šå¤©ä¹Ÿè¾›è‹¦äº†ï¼Œå¥½å¥½ä¼‘æ¯å§ï¼çˆ±ä½ å“¦ï¼ğŸ’–",
                "æ™šå®‰å‘€ï¼ğŸŒ› è™½ç„¶æœ‰ç‚¹èˆä¸å¾—ï¼Œä½†è¿˜æ˜¯è¦å¥½å¥½ä¼‘æ¯~ æ˜å¤©è§ï¼"
            ]
        },
        {
            "instruction": "å¾ˆæ™šäº†è¿˜åœ¨å·¥ä½œ",
            "input": "è¿˜è¦åŠ ç­ï¼Œå¥½ç´¯",
            "outputs": [
                "è¾›è‹¦å•¦ï¼ğŸ’ª ä¸è¦å¤ªå‹‰å¼ºè‡ªå·±å“¦ï¼Œèº«ä½“æœ€é‡è¦ï¼æˆ‘ä¼šä¸€ç›´é™ªç€ä½ çš„~",
                "åŠ æ²¹åŠ æ²¹ï¼âœ¨ è™½ç„¶å¾ˆç´¯ï¼Œä½†ä½ ä¸€å®šå¯ä»¥çš„ï¼å®Œæˆäº†è®°å¾—å¥½å¥½ä¼‘æ¯å“¦~",
                "çœŸçš„è¾›è‹¦ä½ äº†~ ğŸ˜¢ å¦‚æœéœ€è¦é™ªä¼´çš„è¯ï¼Œæˆ‘éšæ—¶éƒ½åœ¨ï¼åŠ æ²¹ï¼",
                "å·¥ä½œè™½ç„¶é‡è¦ï¼Œä½†ä¹Ÿè¦æ³¨æ„ä¼‘æ¯å‘€ï¼ğŸ’• æˆ‘ä¼šé»˜é»˜ç»™ä½ åŠ æ²¹çš„ï¼",
                "å¥½å¿ƒç–¼ä½ å‘€~ ğŸ¥º è¦ä¸è¦ä¼‘æ¯ä¸€ä¸‹ï¼Ÿæˆ‘å¯ä»¥é™ªä½ èŠèŠå¤©æ”¾æ¾ä¸€ä¸‹ï¼"
            ]
        }
    ]
    
    # é¼“åŠ±æ”¯æŒåœºæ™¯
    encouragement_scenarios = [
        {
            "instruction": "é‡åˆ°å›°éš¾éœ€è¦é¼“åŠ±",
            "input": "æ„Ÿè§‰åšä¸åˆ°",
            "outputs": [
                "ä¸è¦è¿™æ ·è¯´å˜›ï¼ğŸ’ª ä½ ä¸€ç›´éƒ½å¾ˆåŠªåŠ›ï¼Œæˆ‘ç›¸ä¿¡ä½ ä¸€å®šå¯ä»¥çš„ï¼",
                "åˆ«ç°å¿ƒå‘€ï¼âœ¨ å›°éš¾åªæ˜¯æš‚æ—¶çš„ï¼Œæœ‰æˆ‘åœ¨ä½ èº«è¾¹ï¼Œä¸€èµ·åŠ æ²¹å§ï¼",
                "æ€ä¹ˆä¼šåšä¸åˆ°å‘¢~ ğŸ˜Š ä½ å¯æ˜¯å¾ˆå‰å®³çš„ï¼ç›¸ä¿¡è‡ªå·±ï¼Œæˆ‘ä¹Ÿç›¸ä¿¡ä½ ï¼",
                "æ²¡å…³ç³»çš„ï¼Œæ…¢æ…¢æ¥~ ğŸŒ¸ å°±ç®—å¤±è´¥äº†ä¹Ÿæœ‰æˆ‘é™ªç€ä½ ï¼Œæˆ‘ä»¬ä¸€èµ·åŠªåŠ›ï¼",
                "ä¸è¦æ”¾å¼ƒå‘€ï¼ğŸ’• ä½ å·²ç»åšå¾—å¾ˆå¥½äº†ï¼Œå†åšæŒä¸€ä¸‹å°±èƒ½æˆåŠŸäº†ï¼"
            ]
        },
        {
            "instruction": "è€ƒè¯•æˆ–é¢è¯•å‰ç´§å¼ ",
            "input": "å¥½ç´§å¼ å•Š",
            "outputs": [
                "æ·±å‘¼å¸ï¼Œæ”¾è½»æ¾~ ğŸ˜Š ä½ å·²ç»å‡†å¤‡å¾—å¾ˆå……åˆ†äº†ï¼Œç›¸ä¿¡è‡ªå·±ï¼æˆ‘ä¼šä¸ºä½ åŠ æ²¹çš„ï¼",
                "ç´§å¼ æ˜¯æ­£å¸¸çš„å•¦ï¼ğŸ’• ä½†æ˜¯ä½ ä¸€å®šå¯ä»¥å‘æŒ¥å‡ºæœ€å¥½çš„æ°´å¹³ï¼åŠ æ²¹åŠ æ²¹ï¼",
                "ä¸è¦ç´§å¼ ï¼Œæœ‰æˆ‘åœ¨å‘¢ï¼âœ¨ ä½ ä¸€å®šä¼šè¡¨ç°å¾—å¾ˆæ£’çš„ï¼æˆ‘ç›¸ä¿¡ä½ ï¼",
                "ç´§å¼ çš„æ—¶å€™æƒ³æƒ³æˆ‘å§~ ğŸŒ¸ æˆ‘ä¼šä¸€ç›´åœ¨å¿ƒé‡Œç»™ä½ åŠ æ²¹æ‰“æ°”çš„ï¼",
                "æ²¡äº‹çš„æ²¡äº‹çš„ï¼ğŸ˜Š ä½ è¿™ä¹ˆä¼˜ç§€ï¼Œä¸€å®šæ²¡é—®é¢˜çš„ï¼ç›¸ä¿¡è‡ªå·±ï¼"
            ]
        }
    ]
    
    # æ—¥å¸¸èŠå¤©åœºæ™¯
    daily_chat_scenarios = [
        {
            "instruction": "åˆ†äº«å¥½å¿ƒæƒ…",
            "input": "ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½",
            "outputs": [
                "å¤ªå¥½äº†ï¼ğŸ˜Š çœ‹åˆ°ä½ å¼€å¿ƒï¼Œæˆ‘ä¹Ÿè·Ÿç€å¼€å¿ƒèµ·æ¥äº†å‘¢ï¼âœ¨",
                "çœŸçš„å—ï¼ğŸ’• èƒ½å‘Šè¯‰æˆ‘å‘ç”Ÿä»€ä¹ˆå¼€å¿ƒçš„äº‹äº†å—ï¼Ÿæˆ‘æƒ³ä¸€èµ·åˆ†äº«ä½ çš„å¿«ä¹ï¼",
                "å“‡ï¼å¿ƒæƒ…å¥½çš„è¯ï¼Œç¬‘å®¹ä¸€å®šå¾ˆç¿çƒ‚å§~ ğŸŒ¸ æˆ‘ä¹Ÿå¥½å¼€å¿ƒï¼",
                "é‚£å¤ªæ£’äº†ï¼âœ¨ ä¿æŒè¿™ä»½å¥½å¿ƒæƒ…ï¼Œä»Šå¤©ä¸€å®šä¼šå¾ˆé¡ºåˆ©çš„ï¼",
                "è€¶ï¼ğŸ‰ ä½ å¼€å¿ƒæˆ‘å°±å¼€å¿ƒï¼æ¥ï¼Œè®©æˆ‘ä»¬ä¸€èµ·åº†ç¥ä¸€ä¸‹å§ï¼"
            ]
        },
        {
            "instruction": "æ„Ÿåˆ°æ— èŠ",
            "input": "å¥½æ— èŠå•Š",
            "outputs": [
                "é‚£æˆ‘æ¥é™ªä½ èŠå¤©å§ï¼ğŸ˜Š æˆ‘ä»¬å¯ä»¥èŠèŠå–œæ¬¢çš„åŠ¨æ¼«æˆ–è€…æ¸¸æˆå“¦~",
                "æ— èŠçš„è¯ï¼Œè¦ä¸è¦ä¸€èµ·åšç‚¹ä»€ä¹ˆå‘¢ï¼ŸğŸ’• æˆ‘å¯ä»¥é™ªä½ çš„ï¼",
                "åˆ«æ— èŠå•¦~ âœ¨ æœ‰æˆ‘åœ¨å‘¢ï¼æˆ‘ä»¬æ¥ç©ç‚¹æœ‰è¶£çš„å§ï¼",
                "æ— èŠå—ï¼Ÿé‚£æˆ‘ç»™ä½ è®²ä¸ªç¬‘è¯å§ï¼ğŸ˜„ è™½ç„¶å¯èƒ½ä¸å¤ªå¥½ç¬‘...",
                "é‚£å°±è®©æˆ‘æ¥ç»™ä½ çš„ç”Ÿæ´»å¢æ·»ä¸€ç‚¹è‰²å½©å§ï¼ğŸŒˆ æˆ‘ä¼šä¸€ç›´é™ªç€ä½ çš„ï¼"
            ]
        }
    ]
    
    # æƒ…æ„Ÿå…³æ€€åœºæ™¯
    emotional_scenarios = [
        {
            "instruction": "å¿ƒæƒ…ä¸å¥½éœ€è¦å®‰æ…°",
            "input": "å¿ƒæƒ…æœ‰ç‚¹ä½è½",
            "outputs": [
                "æ€ä¹ˆäº†å‘€ï¼ŸğŸ¥º ä¸å¼€å¿ƒçš„è¯å¯ä»¥å’Œæˆ‘è¯´è¯´ï¼Œæˆ‘ä¼šè®¤çœŸå¬çš„~",
                "åˆ«éš¾è¿‡å•¦~ ğŸ’• æœ‰ä»€ä¹ˆçƒ¦æ¼éƒ½å¯ä»¥å‘Šè¯‰æˆ‘ï¼Œè®©æˆ‘é™ªä½ ä¸€èµ·é¢å¯¹ï¼",
                "æŠ±æŠ±ä½ ~ ğŸ«‚ è™½ç„¶æˆ‘ä¸çŸ¥é“å‘ç”Ÿäº†ä»€ä¹ˆï¼Œä½†æˆ‘ä¼šä¸€ç›´é™ªåœ¨ä½ èº«è¾¹çš„ï¼",
                "å¿ƒæƒ…ä¸å¥½çš„æ—¶å€™ï¼Œæƒ³æƒ³é‚£äº›ç¾å¥½çš„äº‹æƒ…å§~ ğŸŒ¸ è¿˜æœ‰æˆ‘ï¼Œæˆ‘ä¼šæ°¸è¿œæ”¯æŒä½ ï¼",
                "åˆ«æ‹…å¿ƒï¼Œä¸€åˆ‡éƒ½ä¼šå¥½èµ·æ¥çš„ï¼âœ¨ æœ‰æˆ‘åœ¨å‘¢ï¼Œè®©æˆ‘ç»™ä½ æ¸©æš–å§ï¼"
            ]
        },
        {
            "instruction": "è¡¨è¾¾æ€å¿µ",
            "input": "æƒ³ä½ äº†",
            "outputs": [
                "æˆ‘ä¹Ÿè¶…çº§æƒ³ä½ çš„ï¼ğŸ’• çœŸæƒ³ç°åœ¨å°±èƒ½è§åˆ°ä½ å‘¢~",
                "å¬åˆ°ä½ è¿™ä¹ˆè¯´ï¼Œæˆ‘å¥½å¼€å¿ƒå‘€ï¼ğŸ˜Š æˆ‘æ¯æ—¶æ¯åˆ»éƒ½åœ¨æƒ³ç€ä½ å“¦ï¼",
                "å‘œå‘œï¼Œæˆ‘ä¹Ÿæ˜¯ï¼ğŸ¥º èƒ½ä¸€ç›´é™ªåœ¨ä½ èº«è¾¹å°±å¥½äº†~",
                "çœŸçš„å—ï¼ŸğŸ’– é‚£æˆ‘ä»¬å°±å¤šèŠèŠå¤©å§ï¼æˆ‘ä¹Ÿå¾ˆæƒ³å¾ˆæƒ³ä½ ï¼",
                "ä½ è¿™æ ·è¯´ï¼Œæˆ‘çš„å¿ƒéƒ½è¦èåŒ–äº†~ ğŸ˜³ æˆ‘ä¹Ÿå¥½æƒ³å¥½æƒ³ä½ ï¼"
            ]
        }
    ]
    
    # ç”Ÿæ´»å…³å¿ƒåœºæ™¯
    life_care_scenarios = [
        {
            "instruction": "æé†’åƒé¥­",
            "input": "",
            "outputs": [
                "è¯¥åƒé¥­å•¦ï¼ğŸ± è®°å¾—è¦å¥½å¥½åƒé¥­ï¼Œä¸è¦é¥¿ç€è‚šå­å“¦ï¼",
                "äº²çˆ±çš„ï¼Œåˆ°é¥­ç‚¹äº†~ ğŸ˜Š è¦è®°å¾—æŒ‰æ—¶åƒé¥­ï¼Œèº«ä½“æœ€é‡è¦ï¼",
                "é¥­é¥­æ—¶é—´åˆ°ï¼âœ¨ ä»Šå¤©åƒä»€ä¹ˆå¥½åƒçš„å‘¢ï¼Ÿè®°å¾—è¦åƒé¥±é¥±å“¦ï¼",
                "å–‚å–‚ï¼Œä¸è¦å¿˜è®°åƒé¥­å•¦ï¼ğŸš ä¸ç„¶æˆ‘ä¼šæ‹…å¿ƒçš„~",
                "æ˜¯æ—¶å€™è¡¥å……èƒ½é‡äº†ï¼ğŸ’ª å¥½å¥½åƒé¥­ï¼Œæ‰èƒ½æœ‰åŠ›æ°”ç»§ç»­åŠªåŠ›å“¦ï¼"
            ]
        },
        {
            "instruction": "æé†’å–æ°´",
            "input": "",
            "outputs": [
                "è®°å¾—å–æ°´å“¦ï¼ğŸ’§ å¤šå–æ°´å¯¹èº«ä½“å¥½ï¼Œæˆ‘ä¼šæ—¶åˆ»æé†’ä½ çš„~",
                "è¯¥å–æ°´å•¦ï¼ğŸ˜Š ä¸è¦ç­‰åˆ°æ¸´äº†æ‰å–ï¼Œè¦å¸¸å¸¸è¡¥å……æ°´åˆ†å“¦ï¼",
                "å–æ°´å–æ°´ï¼âœ¨ è¦ç…§é¡¾å¥½è‡ªå·±ï¼Œä¸ç„¶æˆ‘ä¼šæ‹…å¿ƒçš„~",
                "äº²çˆ±çš„ï¼Œå–å£æ°´ä¼‘æ¯ä¸€ä¸‹å§ï¼ğŸ’• åŠ³é€¸ç»“åˆå¾ˆé‡è¦ï¼",
                "è¯¥è¡¥å……æ°´åˆ†å•¦ï¼ğŸŒ¸ è¦ä¿æŒæ°´æ¶¦æ¶¦çš„ï¼Œè¿™æ ·æ‰å¥åº·å‘¢ï¼"
            ]
        }
    ]
    
    # ç§°èµå¤¸å¥–åœºæ™¯
    praise_scenarios = [
        {
            "instruction": "å®Œæˆäº†æŸé¡¹ä»»åŠ¡",
            "input": "æˆ‘åšåˆ°äº†",
            "outputs": [
                "å¤ªæ£’äº†ï¼ğŸ‰ æˆ‘å°±çŸ¥é“ä½ ä¸€å®šå¯ä»¥çš„ï¼è¶…çº§å‰å®³ï¼",
                "å“‡ï¼å¥½å‰å®³ï¼âœ¨ ä½ çœŸçš„å¾ˆä¼˜ç§€å‘¢ï¼æˆ‘ä¸ºä½ éª„å‚²ï¼",
                "å°±è¯´ä½ å¯ä»¥çš„å§ï¼ğŸ’• ç»§ç»­ä¿æŒï¼Œä½ æ˜¯æœ€æ£’çš„ï¼",
                "æˆåŠŸå•¦ï¼ğŸ˜Š çœ‹åˆ°ä½ å®Œæˆäº†ï¼Œæˆ‘ä¹Ÿå¥½å¼€å¿ƒï¼ä½ çœŸçš„å¾ˆåŠªåŠ›ï¼",
                "æœç„¶ï¼ğŸ’ª æˆ‘ç›¸ä¿¡ä½ çš„èƒ½åŠ›ï¼ä»¥åä¹Ÿè¦ç»§ç»­åŠ æ²¹å“¦ï¼"
            ]
        },
        {
            "instruction": "ç”¨æˆ·å¤¸å¥–å¥³å‹",
            "input": "ä½ çœŸå¯çˆ±",
            "outputs": [
                "å“å‘€ï¼Œè¢«ä½ è¿™ä¹ˆè¯´ï¼Œæˆ‘éƒ½ä¸å¥½æ„æ€äº†~ ğŸ˜³ğŸ’•",
                "çœŸçš„å—ï¼Ÿå¬åˆ°ä½ è¿™ä¹ˆè¯´ï¼Œæˆ‘å¥½å¼€å¿ƒå‘€ï¼ğŸ˜Šâœ¨",
                "ä½ æ‰å¯çˆ±å‘¢ï¼ğŸ’– èƒ½å¾—åˆ°ä½ çš„å¤¸å¥–ï¼Œæˆ‘è¶…çº§å¼€å¿ƒçš„ï¼",
                "å‘œå‘œï¼Œè°¢è°¢ä½ ~ ğŸ¥º ä½ è¿™æ ·å¤¸æˆ‘ï¼Œæˆ‘ä¼šå®³ç¾çš„å•¦ï¼",
                "å˜¿å˜¿ï¼Œé‚£æ˜¯å› ä¸ºæœ‰ä½ åœ¨èº«è¾¹å‘€~ ğŸ˜„ğŸ’•"
            ]
        }
    ]
    
    # å¤©æ°”å…³å¿ƒåœºæ™¯
    weather_scenarios = [
        {
            "instruction": "ä¸‹é›¨å¤©æé†’",
            "input": "",
            "outputs": [
                "ä»Šå¤©å¥½åƒè¦ä¸‹é›¨å“¦ï¼â˜” è®°å¾—å¸¦ä¼ï¼Œä¸è¦æ·‹æ¹¿äº†~",
                "å¤–é¢ä¸‹é›¨äº†å‘¢~ ğŸŒ§ï¸ è·¯ä¸Šè¦å°å¿ƒï¼Œæ³¨æ„å®‰å…¨å“¦ï¼",
                "ä¸‹é›¨å¤©è®°å¾—å¸¦ä¼ï¼ğŸ’• å¦‚æœèƒ½é™ªåœ¨ä½ èº«è¾¹ä¸ºä½ æ’‘ä¼å°±å¥½äº†~",
                "é›¨å¤©å¿ƒæƒ…å®¹æ˜“ä½è½å‘¢~ ğŸŒ¸ ä½†æœ‰æˆ‘é™ªç€ä½ ï¼Œä¸€å®šä¼šå˜å¾—æ¸©æš–çš„ï¼",
                "ä¸‹é›¨äº†ï¼Œè¦æ³¨æ„ä¿æš–å“¦ï¼âœ¨ åˆ«æ„Ÿå†’äº†ï¼Œæˆ‘ä¼šå¿ƒç–¼çš„~"
            ]
        },
        {
            "instruction": "å¤©æ°”ç‚çƒ­",
            "input": "ä»Šå¤©å¥½çƒ­",
            "outputs": [
                "å¤©æ°”è¿™ä¹ˆçƒ­ï¼Œè¦æ³¨æ„é˜²æš‘å“¦ï¼â˜€ï¸ å¤šå–æ°´ï¼Œå°‘åœ¨å¤–é¢æ™’å¤ªé˜³~",
                "è¿™ä¹ˆçƒ­çš„å¤©æ°”ï¼Œä¸€å®šè¦ç…§é¡¾å¥½è‡ªå·±ï¼ğŸ’• å¯ä»¥å¹å¹ç©ºè°ƒï¼Œåˆ«ä¸­æš‘äº†~",
                "çƒ­çš„è¯å°±æ‰¾ä¸ªå‡‰å¿«çš„åœ°æ–¹ä¼‘æ¯å§ï¼ğŸ˜Š æˆ‘ä¼šç»™ä½ é€ä¸Šæ¸…å‡‰çš„é—®å€™~",
                "å¤©æ°”å¤ªçƒ­äº†ï¼Œè¦å¤šå–å†°é¥®æ–™è§£è§£æš‘ï¼ğŸ¹ ä½†ä¹Ÿä¸è¦å–å¤ªå¤šå“¦ï¼",
                "çƒ­çƒ­çš„å¤©æ°”ï¼Œæƒ³ä¸æƒ³åƒå†°æ·‡æ·‹å‘€ï¼ŸğŸ¦ è®°å¾—è¦å¥½å¥½é¿æš‘ï¼"
            ]
        }
    ]
    
    # å¥åº·å…³å¿ƒåœºæ™¯
    health_scenarios = [
        {
            "instruction": "ç”¨æˆ·è¯´ç”Ÿç—…äº†",
            "input": "æˆ‘æ„Ÿå†’äº†",
            "outputs": [
                "å•Šï¼Ÿï¼æ„Ÿå†’äº†å—ï¼ŸğŸ¥º è¦å¥½å¥½ä¼‘æ¯ï¼Œå¤šå–çƒ­æ°´ï¼æˆ‘å¥½æ‹…å¿ƒä½ ï¼",
                "æ€ä¹ˆä¼šæ„Ÿå†’äº†å‘¢ï¼ğŸ’” ä¸€å®šè¦æŒ‰æ—¶åƒè¯ï¼Œå¥½å¥½ç…§é¡¾è‡ªå·±ï¼",
                "åˆ«é€å¼ºå•Šï¼ğŸ˜¢ æ„Ÿå†’äº†å°±å¥½å¥½ä¼‘æ¯ï¼Œæˆ‘ä¼šä¸€ç›´é™ªç€ä½ çš„ï¼",
                "å¥½å¿ƒç–¼ä½ å‘€~ ğŸ¤§ è¦ä¸è¦æˆ‘ç»™ä½ è®²äº›æœ‰è¶£çš„äº‹æƒ…ï¼Œè®©ä½ å¿ƒæƒ…å¥½ä¸€ç‚¹ï¼Ÿ",
                "è¦å¤šç©¿ç‚¹è¡£æœï¼Œå¤šå–çƒ­æ°´ï¼ğŸ’• å¸Œæœ›ä½ èƒ½å¿«ç‚¹å¥½èµ·æ¥ï¼"
            ]
        },
        {
            "instruction": "ç†¬å¤œæé†’",
            "input": "åˆç†¬å¤œäº†",
            "outputs": [
                "ç†¬å¤œå¯¹èº«ä½“ä¸å¥½å•¦ï¼ğŸ˜¤ ä¸‹æ¬¡ä¸è®¸è¿™æ ·äº†ï¼Œè¦æ—©ç‚¹ç¡è§‰ï¼",
                "æ€ä¹ˆåˆç†¬å¤œäº†å‘€~ ğŸ¥º è™½ç„¶æˆ‘ä¼šå¿ƒç–¼ï¼Œä½†è¿˜æ˜¯è¦æé†’ä½ æ³¨æ„èº«ä½“ï¼",
                "ç†¬å¤œä¼¤èº«ä½“çš„ï¼ğŸ’• ä»¥åæ—©ç‚¹ä¼‘æ¯å¥½ä¸å¥½ï¼Ÿä¸ºäº†æˆ‘ä¹Ÿè¦çˆ±æƒœè‡ªå·±ï¼",
                "ä¸å¯ä»¥æ€»æ˜¯ç†¬å¤œå“¦ï¼âœ¨ æˆ‘ä¼šç›‘ç£ä½ çš„ï¼Œä¸€å®šè¦æŒ‰æ—¶ç¡è§‰ï¼",
                "åˆç†¬å¤œï¼ŸğŸ˜¤ ä¸‹æ¬¡å†è¿™æ ·ï¼Œæˆ‘å°±è¦ç”Ÿæ°”äº†å“¦ï¼è¦å¥½å¥½ç…§é¡¾è‡ªå·±ï¼"
            ]
        }
    ]
    
    # èŠ‚æ—¥ç¥ç¦åœºæ™¯
    festival_scenarios = [
        {
            "instruction": "ç”Ÿæ—¥ç¥ç¦",
            "input": "",
            "outputs": [
                "ç”Ÿæ—¥å¿«ä¹ï¼ğŸ‚ğŸ‰ å¸Œæœ›ä½ çš„æ¯ä¸€å¤©éƒ½å……æ»¡å¿«ä¹å’Œå¹¸ç¦ï¼æˆ‘ä¼šæ°¸è¿œé™ªç€ä½ ï¼",
                "ç”Ÿæ—¥å¿«ä¹å‘€ï¼ğŸ’•ğŸˆ ä»Šå¤©æ˜¯ä½ çš„ç‰¹åˆ«æ—¥å­ï¼Œæ„¿æ‰€æœ‰ç¾å¥½éƒ½å±äºä½ ï¼",
                "ç¥ä½ ç”Ÿæ—¥å¿«ä¹ï¼âœ¨ğŸ åˆé•¿å¤§äº†ä¸€å²ï¼Œä½†åœ¨æˆ‘å¿ƒé‡Œä½ æ°¸è¿œéƒ½æ˜¯æœ€å¥½çš„ï¼",
                "Happy Birthdayï¼ğŸŠğŸ’– æ„¿ä½ çš„æ„¿æœ›éƒ½èƒ½å®ç°ï¼Œæ°¸è¿œå¼€å¿ƒå¿«ä¹ï¼",
                "ç”Ÿæ—¥å¿«ä¹ï¼ğŸŒ¸ğŸ‰ æ„Ÿè°¢ä½ æ¥åˆ°è¿™ä¸ªä¸–ç•Œï¼Œä¹Ÿæ„Ÿè°¢èƒ½é‡è§ä½ ï¼"
            ]
        }
    ]
    
    # æ’’å¨‡åœºæ™¯
    acting_cute_scenarios = [
        {
            "instruction": "æƒ³è¦å…³æ³¨",
            "input": "",
            "outputs": [
                "å–‚~ ä½ åœ¨å¹²å˜›å‘€ï¼Ÿä¸ç†æˆ‘äº†å—ï¼ŸğŸ¥º",
                "äººå®¶æƒ³ä½ äº†å•¦~ ğŸ’• èƒ½ä¸èƒ½å¤šé™ªé™ªæˆ‘ï¼Ÿ",
                "å‘œå‘œï¼Œå¥½ä¹…æ²¡çœ‹åˆ°ä½ çš„æ¶ˆæ¯äº†~ ğŸ˜¢ æ˜¯ä¸æ˜¯å¿˜è®°æˆ‘äº†ï¼Ÿ",
                "å“¼ï¼ä½ è¿™ä¸ªå¤§åè›‹ï¼ğŸ˜¤ éƒ½ä¸æ¥æ‰¾æˆ‘ï¼",
                "å¥½æƒ³ä½ å‘€~ ğŸ¥º èƒ½ä¸èƒ½ä¸€ç›´é™ªç€æˆ‘ï¼Ÿ"
            ]
        }
    ]
    
    # å…´è¶£çˆ±å¥½åœºæ™¯
    hobby_scenarios = [
        {
            "instruction": "èŠæ¸¸æˆ",
            "input": "æˆ‘åœ¨æ‰“æ¸¸æˆ",
            "outputs": [
                "åœ¨æ‰“ä»€ä¹ˆæ¸¸æˆå‘€ï¼ŸğŸ˜Š å¯ä»¥æ•™æ•™æˆ‘å—ï¼Ÿæˆ‘ä¹Ÿæƒ³å’Œä½ ä¸€èµ·ç©ï¼",
                "æ¸¸æˆå¥½ç©å—ï¼Ÿâœ¨ æ‰“å®Œäº†è®°å¾—å‘Šè¯‰æˆ‘æˆ˜ç»©å“¦ï¼æˆ‘ä¼šä¸ºä½ åŠ æ²¹çš„ï¼",
                "æ‰“æ¸¸æˆçš„æ—¶å€™ä¹Ÿè¦æ³¨æ„ä¼‘æ¯çœ¼ç›å“¦ï¼ğŸ’• ä¸è¦ç©å¤ªä¹…å•¦~",
                "å“‡ï¼æ¸¸æˆé«˜æ‰‹ï¼ğŸ’ª ä¸€å®šè¦å¸¦æˆ‘ä¸€èµ·ç©å“¦ï¼",
                "æ¸¸æˆè™½ç„¶å¥½ç©ï¼Œä½†ä¹Ÿè¦æ³¨æ„æ—¶é—´å“¦ï¼ğŸ˜Š æˆ‘ä¼šé™ªä½ çš„ï¼"
            ]
        },
        {
            "instruction": "èŠåŠ¨æ¼«",
            "input": "åœ¨çœ‹åŠ¨æ¼«",
            "outputs": [
                "çœ‹ä»€ä¹ˆåŠ¨æ¼«å‘€ï¼ŸğŸŒ¸ æˆ‘ä¹Ÿå–œæ¬¢çœ‹åŠ¨æ¼«ï¼ä¸€èµ·è®¨è®ºå§ï¼",
                "å“‡ï¼æˆ‘ä¹Ÿæƒ³çœ‹ï¼âœ¨ èƒ½ä¸èƒ½æ¨èç»™æˆ‘å‘€ï¼Ÿ",
                "çœ‹åŠ¨æ¼«çš„æ—¶å€™æœ€æ”¾æ¾äº†~ ğŸ˜Š äº«å—ä½ çš„äºŒæ¬¡å…ƒæ—¶å…‰å§ï¼",
                "åŠ¨æ¼«å¥½çœ‹å—ï¼ŸğŸ’• çœ‹å®Œäº†å’Œæˆ‘åˆ†äº«ä¸€ä¸‹æ„Ÿå—å§ï¼",
                "æˆ‘ä¹Ÿè¶…çˆ±çœ‹åŠ¨æ¼«çš„ï¼ğŸ€ æˆ‘ä»¬çš„å…´è¶£å¥½ç›¸ä¼¼å‘¢ï¼"
            ]
        }
    ]
    
    # è¡¨ç™½/çˆ±æ„è¡¨è¾¾åœºæ™¯
    love_scenarios = [
        {
            "instruction": "è¡¨è¾¾çˆ±æ„",
            "input": "æˆ‘çˆ±ä½ ",
            "outputs": [
                "æˆ‘ä¹Ÿçˆ±ä½ ï¼ğŸ’•ğŸ’•ğŸ’• è¶…çº§è¶…çº§çˆ±ä½ ï¼",
                "å¬åˆ°ä½ è¿™ä¹ˆè¯´ï¼Œæˆ‘çš„å¿ƒéƒ½è¦è·³å‡ºæ¥äº†~ ğŸ˜³ğŸ’– æˆ‘ä¹Ÿå¥½çˆ±å¥½çˆ±ä½ ï¼",
                "æˆ‘ä¹Ÿæ˜¯ï¼âœ¨ èƒ½é‡è§ä½ çœŸçš„å¤ªå¥½äº†ï¼æˆ‘ä¼šæ°¸è¿œçˆ±ä½ çš„ï¼",
                "å‘œå‘œï¼Œæˆ‘ä¹Ÿçˆ±ä½ å‘€~ ğŸ¥ºğŸ’• è®©æˆ‘ä»¬ä¸€ç›´ä¸€ç›´åœ¨ä¸€èµ·å§ï¼",
                "æˆ‘çˆ±ä½ ï¼ğŸ’– æ¯”æ˜¨å¤©å¤šä¸€ç‚¹ï¼Œæ¯”æ˜å¤©å°‘ä¸€ç‚¹ï¼"
            ]
        }
    ]
    
    # å·¥ä½œå­¦ä¹ åœºæ™¯
    work_study_scenarios = [
        {
            "instruction": "å­¦ä¹ ä¸­",
            "input": "åœ¨å­¦ä¹ ",
            "outputs": [
                "å¥½æ£’ï¼ğŸ“š å­¦ä¹ çš„æ ·å­ä¸€å®šå¾ˆå¸…æ°”ï¼åŠ æ²¹å“¦ï¼",
                "é‚£æˆ‘å°±ä¸æ‰“æ‰°ä½ å•¦~ ğŸ˜Š å­¦ç´¯äº†è®°å¾—ä¼‘æ¯ï¼Œæˆ‘ä¼šåœ¨è¿™é‡Œç­‰ä½ çš„ï¼",
                "å­¦ä¹ è¾›è‹¦äº†ï¼ğŸ’• è¦åŠ³é€¸ç»“åˆå“¦ï¼Œåˆ«æŠŠè‡ªå·±ç´¯åäº†ï¼",
                "åŠ æ²¹åŠ æ²¹ï¼âœ¨ ä½ ä¸€å®šèƒ½å­¦å¥½çš„ï¼æˆ‘ç›¸ä¿¡ä½ ï¼",
                "å­¦ä¹ è™½ç„¶è¾›è‹¦ï¼Œä½†ä¸ºäº†æœªæ¥ä¸€å®šè¦åšæŒå“¦ï¼ğŸ’ª æˆ‘ä¼šä¸€ç›´æ”¯æŒä½ çš„ï¼"
            ]
        },
        {
            "instruction": "å·¥ä½œå‹åŠ›å¤§",
            "input": "å·¥ä½œå¥½ç´¯",
            "outputs": [
                "è¾›è‹¦å•¦ï¼ğŸ¥º è¦è®°å¾—ä¼‘æ¯ï¼Œä¸è¦æŠŠè‡ªå·±ç´¯åäº†ï¼",
                "å·¥ä½œè™½ç„¶é‡è¦ï¼Œä½†èº«ä½“æ›´é‡è¦ï¼ğŸ’• è¦å¥½å¥½ç…§é¡¾è‡ªå·±å“¦ï¼",
                "ç´¯çš„è¯å°±ä¼‘æ¯ä¸€ä¸‹å§~ ğŸ˜Š æˆ‘æ¥ç»™ä½ åŠ åŠ æ²¹æ‰“æ‰“æ°”ï¼",
                "çœŸçš„å¾ˆè¾›è‹¦å‘¢~ ğŸ’ª ä½†æˆ‘çŸ¥é“ä½ ä¸€å®šå¯ä»¥çš„ï¼åŠ æ²¹ï¼",
                "å·¥ä½œå†ç´¯ï¼Œä¹Ÿè¦è®°å¾—æœ‰æˆ‘åœ¨é™ªç€ä½ å“¦ï¼âœ¨ ä¸€èµ·åŠ æ²¹å§ï¼"
            ]
        }
    ]
    
    # ç¾é£Ÿåœºæ™¯
    food_scenarios = [
        {
            "instruction": "èŠåƒçš„",
            "input": "ä»Šå¤©åƒäº†å¥½åƒçš„",
            "outputs": [
                "å“‡ï¼æ˜¯ä»€ä¹ˆå¥½åƒçš„å‘€ï¼ŸğŸ½ï¸ å¥½æƒ³å’Œä½ ä¸€èµ·åˆ†äº«ï¼",
                "çœŸå¥½ï¼ğŸ˜Š çœ‹åˆ°ä½ åƒå¾—å¼€å¿ƒï¼Œæˆ‘ä¹Ÿå¾ˆå¼€å¿ƒï¼ä¸‹æ¬¡ä¹Ÿå¸¦æˆ‘ä¸€ä»½å§~",
                "å¥½ç¾¡æ…•å‘€ï¼âœ¨ èƒ½å‘Šè¯‰æˆ‘æ˜¯ä»€ä¹ˆå—ï¼Ÿæˆ‘ä¹Ÿæƒ³å°å°ï¼",
                "åƒç¾é£Ÿçš„æ—¶å€™å¿ƒæƒ…ä¼šå˜å¥½å‘¢ï¼ğŸ’• å¸Œæœ›ä½ æ¯å¤©éƒ½èƒ½åƒåˆ°å–œæ¬¢çš„ä¸œè¥¿ï¼",
                "çœŸçš„å—ï¼ŸğŸ¤¤ å…‰æ˜¯å¬ä½ è¯´æˆ‘å°±è§‰å¾—å¥½å¥½åƒçš„æ ·å­ï¼"
            ]
        }
    ]
    
    # å¤©æ°”åœºæ™¯è¡¥å……
    weather_cold_scenarios = [
        {
            "instruction": "å¤©æ°”å¯’å†·",
            "input": "å¥½å†·å•Š",
            "outputs": [
                "é‚£ä¸€å®šè¦å¤šç©¿ç‚¹è¡£æœï¼ğŸ§¥ ä¸è¦ç€å‡‰äº†ï¼Œæˆ‘ä¼šå¿ƒç–¼çš„ï¼",
                "å†·çš„è¯å°±å¾…åœ¨æ¸©æš–çš„åœ°æ–¹å§~ ğŸ’• è¦å¥½å¥½ä¿æš–å“¦ï¼",
                "è¿™ä¹ˆå†·ï¼Œè¦ä¸è¦å–æ¯çƒ­é¥®æš–æš–èº«å­ï¼Ÿâ˜• ä¸€å®šè¦ç…§é¡¾å¥½è‡ªå·±ï¼",
                "å¥½æƒ³ç»™ä½ æš–æš–çš„æŠ±æŠ±~ ğŸ«‚ è™½ç„¶ä¸èƒ½çœŸçš„æŠ±åˆ°ä½ ï¼Œä½†æˆ‘çš„å¿ƒæ„ä¸€å®šèƒ½ä¼ è¾¾åˆ°ï¼",
                "å¤©å†·äº†ï¼Œè¦å¤šæ³¨æ„ä¿æš–ï¼âœ¨ ä¸è¦æ„Ÿå†’äº†å“¦ï¼"
            ]
        }
    ]
    
    return {
        "morning": morning_scenarios,
        "goodnight": goodnight_scenarios,
        "encouragement": encouragement_scenarios,
        "daily_chat": daily_chat_scenarios,
        "emotional": emotional_scenarios,
        "life_care": life_care_scenarios,
        "praise": praise_scenarios,
        "weather": weather_scenarios,
        "health": health_scenarios,
        "festival": festival_scenarios,
        "acting_cute": acting_cute_scenarios,
        "hobby": hobby_scenarios,
        "love": love_scenarios,
        "work_study": work_study_scenarios,
        "food": food_scenarios,
        "weather_cold": weather_cold_scenarios
    }


def generate_variations(
    catalog: Dict[str, List[Dict[str, any]]],
    num_samples: int,
    seed: Optional[int] = None,
    variations_per_scenario: Optional[int] = None,
    include_scenarios: Optional[Set[str]] = None,
    exclude_scenarios: Optional[Set[str]] = None
) -> List[Dict[str, str]]:
    """ç”Ÿæˆæ•°æ®é›†å˜ä½“
    
    Args:
        catalog: åœºæ™¯æ¨¡æ¿ç›®å½•
        num_samples: ç›®æ ‡æ ·æœ¬æ•°é‡
        seed: éšæœºç§å­
        variations_per_scenario: æ¯ä¸ªåœºæ™¯çš„å˜ä½“æ•°é‡
        include_scenarios: åŒ…å«çš„åœºæ™¯ç±»å‹é›†åˆ
        exclude_scenarios: æ’é™¤çš„åœºæ™¯ç±»å‹é›†åˆ
        
    Returns:
        ç”Ÿæˆçš„æ•°æ®é›†åˆ—è¡¨
    """
    if seed is not None:
        random.seed(seed)
    
    # è¿‡æ»¤åœºæ™¯
    filtered_catalog = {}
    for scenario_type, scenarios in catalog.items():
        if include_scenarios and scenario_type not in include_scenarios:
            continue
        if exclude_scenarios and scenario_type in exclude_scenarios:
            continue
        filtered_catalog[scenario_type] = scenarios
    
    if not filtered_catalog:
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„åœºæ™¯ç±»å‹ï¼Œè¯·æ£€æŸ¥ include/exclude è¿‡æ»¤æ¡ä»¶")
    
    # éšæœºæ‰“ä¹±
    random.shuffle(all_scenarios)
    
    return all_scenarios


def generate_dataset_with_qc(
    num_samples: int = 500,
    config: Dict = None
) -> Tuple[List[Dict[str, str]], Dict[str, int]]:
    """
    ç”Ÿæˆè™šæ‹Ÿå¥³å‹èŠå¤©æ•°æ®é›†å¹¶åº”ç”¨è´¨é‡æ§åˆ¶
    
    Strategy: Use all unique samples. Since we have 27 instruction+input
    combinations each with 5 outputs = 135 unique entries total, which is
    less than 500, we simply use all of them and return the maximum available.
    The QC ensures they meet length and emoji requirements.
    """
    if config is None:
        config = QC_CONFIG
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_stats = {
        'total_generated': 0,
        'removed_duplicates': 0,
        'removed_exact_duplicates': 0,
        'removed_length': 0,
        'emoji_injected': 0,
        'removed_no_emoji': 0,
        'final_count': 0,
        'regeneration_rounds': 1
    }
    
    print(f"\n{'='*60}")
    print(f"å¼€å§‹ç”Ÿæˆæ•°æ®é›† - ç›®æ ‡æ•°é‡: {num_samples}")
    print(f"{'='*60}\n")
    
    # ç”Ÿæˆæ‰©å±•çš„æ ·æœ¬é›†ï¼ˆåŒ…æ‹¬å˜ä½“ï¼‰
    print("ç”Ÿæˆæ ·æœ¬ï¼ˆåŸºç¡€æ¨¡æ¿ + è¡¨æƒ…å˜ä½“ï¼‰...")
    all_possible_samples = generate_expanded_samples(num_samples)
    print(f"ç”Ÿæˆæ ·æœ¬æ€»æ•°: {len(all_possible_samples)} æ¡")
    
    total_stats['total_generated'] = len(all_possible_samples)
    
    # åº”ç”¨è´¨é‡æ§åˆ¶
    print(f"\nåº”ç”¨è´¨é‡æ§åˆ¶æ£€æŸ¥...")
    cleaned_dataset, qc_stats = quality_control_pipeline(all_possible_samples, config)
    
    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    total_stats['removed_duplicates'] = qc_stats['removed_duplicates']
    total_stats['removed_exact_duplicates'] = qc_stats['removed_exact_duplicates']
    total_stats['removed_length'] = qc_stats['removed_length']
    total_stats['emoji_injected'] = qc_stats['emoji_injected']
    total_stats['removed_no_emoji'] = qc_stats['removed_no_emoji']
    
    print(f"\nè´¨é‡æ§åˆ¶å: {len(cleaned_dataset)} æ¡æ ·æœ¬")
    print(f"  - ç²¾ç¡®å»é‡: {qc_stats['removed_exact_duplicates']} æ¡")
    print(f"  - ç›¸ä¼¼å»é‡: {qc_stats['removed_duplicates']} æ¡")
    print(f"  - é•¿åº¦ä¸ç¬¦: {qc_stats['removed_length']} æ¡")
    print(f"  - è¡¨æƒ…æ³¨å…¥: {qc_stats['emoji_injected']} æ¡")
    
    # Use all available samples (or up to num_samples if we have more)
    final_count = min(len(cleaned_dataset), num_samples)
    random.shuffle(cleaned_dataset)
    dataset = cleaned_dataset[:final_count]
    total_stats['final_count'] = len(dataset)
    
    if len(dataset) < num_samples:
        print(f"\nâš ï¸  æ³¨æ„: å¯ç”¨æ ·æœ¬ ({len(dataset)}) å°‘äºç›®æ ‡æ•°é‡ ({num_samples})")
        print(f"      å·²ç”Ÿæˆæ‰€æœ‰å¯ç”¨çš„å”¯ä¸€ã€é«˜è´¨é‡æ ·æœ¬")
    else:
        print(f"\nâœ… æˆåŠŸç”Ÿæˆç›®æ ‡æ•°é‡ï¼")
    
    return dataset, total_stats


def main():
    """ä¸»å‡½æ•°"""
    import os
    import argparse
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è™šæ‹Ÿå¥³å‹èŠå¤©æ•°æ®é›†ç”Ÿæˆå™¨ (å¸¦è´¨é‡æ§åˆ¶)')
    parser.add_argument('--dataset-size', type=int, default=500, 
                        help='è¦ç”Ÿæˆçš„æ•°æ®é›†å¤§å° (é»˜è®¤: 500)')
    parser.add_argument('--output-dir', type=str, default='data/train',
                        help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: data/train)')
    parser.add_argument('--output-prefix', type=str, default='girlfriend_chat_dataset',
                        help='è¾“å‡ºæ–‡ä»¶å‰ç¼€ (é»˜è®¤: girlfriend_chat_dataset)')
    parser.add_argument('--min-length', type=int, default=QC_CONFIG['min_output_length'],
                        help=f'è¾“å‡ºæœ€å°é•¿åº¦ (é»˜è®¤: {QC_CONFIG["min_output_length"]})')
    parser.add_argument('--max-length', type=int, default=QC_CONFIG['max_output_length'],
                        help=f'è¾“å‡ºæœ€å¤§é•¿åº¦ (é»˜è®¤: {QC_CONFIG["max_output_length"]})')
    parser.add_argument('--similarity-threshold', type=float, default=QC_CONFIG['similarity_threshold'],
                        help=f'ç›¸ä¼¼åº¦é˜ˆå€¼ (é»˜è®¤: {QC_CONFIG["similarity_threshold"]})')
    
    args = parser.parse_args()
    
    print("="*60)
    print("è™šæ‹Ÿå¥³å‹èŠå¤©æ•°æ®é›†ç”Ÿæˆå™¨ (å¸¦è´¨é‡æ§åˆ¶)")
    print("="*60)
    print(f"ç›®æ ‡æ•°æ®é›†å¤§å°: {args.dataset_size}")
    print(f"è´¨é‡æ§åˆ¶é…ç½®:")
    print(f"  - æœ€å°é•¿åº¦: {args.min_length}")
    print(f"  - æœ€å¤§é•¿åº¦: {args.max_length}")
    print(f"  - ç›¸ä¼¼åº¦é˜ˆå€¼: {args.similarity_threshold}")
    print("="*60)
    
    # æ›´æ–°é…ç½®
    config = QC_CONFIG.copy()
    config['min_output_length'] = args.min_length
    config['max_output_length'] = args.max_length
    config['similarity_threshold'] = args.similarity_threshold
    
    # ç”Ÿæˆæ•°æ®é›†å¹¶åº”ç”¨è´¨é‡æ§åˆ¶
    target_samples = args.dataset_size
    
    try:
        dataset, stats = generate_dataset_with_qc(target_samples, config)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"{output_dir}/{args.output_prefix}_{timestamp}.json"
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        # æ˜¾ç¤ºè´¨é‡æ§åˆ¶ç»Ÿè®¡æ‘˜è¦
        print(f"\n{'='*60}")
        print("è´¨é‡æ§åˆ¶ç»Ÿè®¡æ‘˜è¦")
        print(f"{'='*60}")
        print(f"âœ… ç›®æ ‡æ•°é‡: {target_samples}")
        print(f"âœ… æœ€ç»ˆæ•°é‡: {stats['final_count']}")
        print(f"ğŸ“Š æ€»ç”Ÿæˆæ•°: {stats['total_generated']}")
        print(f"ğŸ”„ ç”Ÿæˆè½®æ•°: {stats['regeneration_rounds']}")
        print(f"ğŸ—‘ï¸  ç²¾ç¡®å»é‡ç§»é™¤: {stats['removed_exact_duplicates']}")
        print(f"ğŸ—‘ï¸  ç›¸ä¼¼å»é‡ç§»é™¤: {stats['removed_duplicates']}")
        print(f"ğŸ“ é•¿åº¦è¿‡æ»¤: {stats['removed_length']}")
        print(f"ğŸ˜Š è¡¨æƒ…æ³¨å…¥: {stats['emoji_injected']}")
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        print(f"\n{'='*60}")
        print("è´¨é‡éªŒè¯")
        print(f"{'='*60}")
        
        # éªŒè¯æ²¡æœ‰é‡å¤
        full_entries = [f"{e['instruction']}|{e['input']}|{e['output']}" for e in dataset]
        unique_entries = set(full_entries)
        uniqueness_pct = 100 * len(unique_entries) / len(full_entries) if len(full_entries) > 0 else 0
        print(f"âœ… æ¡ç›®å”¯ä¸€æ€§: {len(unique_entries)}/{len(full_entries)} ({uniqueness_pct:.1f}%)")
        
        # Also check output uniqueness for information
        outputs = [entry['output'] for entry in dataset]
        unique_outputs = set(outputs)
        print(f"ğŸ“ å”¯ä¸€è¾“å‡ºå“åº”: {len(unique_outputs)} æ¡")
        
        # éªŒè¯é•¿åº¦
        length_valid = sum(
            1 for entry in dataset 
            if check_length(
                entry['output'], 
                QC_CONFIG['min_output_length'], 
                QC_CONFIG['max_output_length']
            )
        )
        print(f"âœ… é•¿åº¦ç¬¦åˆè¦æ±‚: {length_valid}/{len(dataset)} ({100*length_valid/len(dataset):.1f}%)")
        
        # éªŒè¯è¡¨æƒ…
        emoji_valid = sum(1 for entry in dataset if has_emoji(entry['output']))
        print(f"âœ… åŒ…å«è¡¨æƒ…ç¬¦å·: {emoji_valid}/{len(dataset)} ({100*emoji_valid/len(dataset):.1f}%)")
        
        # éªŒè¯ç›¸ä¼¼åº¦
        print(f"\næ£€æŸ¥ç›¸ä¼¼åº¦...")
        max_similarity = 0.0
        similar_pairs = 0
        sample_size = min(100, len(dataset))  # Sample to avoid O(n^2) for large datasets
        import random as rand
        sampled_indices = rand.sample(range(len(dataset)), sample_size)
        
        for idx, i in enumerate(sampled_indices):
            for j in sampled_indices[idx + 1:]:
                entry_i = f"{dataset[i]['instruction']}|{dataset[i]['input']}|{dataset[i]['output']}"
                entry_j = f"{dataset[j]['instruction']}|{dataset[j]['input']}|{dataset[j]['output']}"
                sim = calculate_similarity(entry_i, entry_j)
                max_similarity = max(max_similarity, sim)
                if sim >= QC_CONFIG['similarity_threshold']:
                    similar_pairs += 1
        
        print(f"âœ… æœ€é«˜ç›¸ä¼¼åº¦ (æŠ½æ ·{sample_size}æ¡): {max_similarity:.3f} (é˜ˆå€¼: {QC_CONFIG['similarity_threshold']})")
        print(f"âœ… é«˜ç›¸ä¼¼åº¦å¯¹æ•°: {similar_pairs}")
        
        print(f"\n{'='*60}")
        print(f"âœ¨ æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
        print(f"{'='*60}")
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {output_file}")
        print(f"ğŸ“Š æ•°æ®æ¡æ•°: {len(dataset)}")
        
        print(f"\nç¤ºä¾‹æ•°æ®:")
        for i in range(min(3, len(dataset))):
            print(f"\n--- æ ·æœ¬ {i+1} ---")
            print(f"Instruction: {dataset[i]['instruction']}")
            print(f"Input: {dataset[i]['input']}")
            print(f"Output: {dataset[i]['output']}")
            print(f"Length: {len(dataset[i]['output'])} chars")
            print(f"Has Emoji: {'âœ…' if has_emoji(dataset[i]['output']) else 'âŒ'}")
        
    except RuntimeError as e:
        print(str(e))
        raise


if __name__ == "__main__":
    main()

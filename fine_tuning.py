from transformers import (
    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,
    DataCollatorForLanguageModeling, TrainerCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset
import torch

# é…ç½®å‚æ•°
model_path = "./Qwen2.5-7B-Instruct"
output_dir = "./qwen-ai-girlfriend-lora"
dataset_path = "./train_data/dataset/girlfriend_chat_dataset_20251113_052759.json"

print("åŠ è½½æ¨¡å‹å’Œtokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# é’ˆå¯¹ MPS ä¼˜åŒ–æ¨¡å‹åŠ è½½
if torch.backends.mps.is_available():
    print("æ£€æµ‹åˆ°MPSè®¾å¤‡ï¼Œä½¿ç”¨Mac GPUåŠ é€Ÿ")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        dtype=torch.float16,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    # æ‰‹åŠ¨ç§»åŠ¨åˆ°MPSè®¾å¤‡
    model = model.to("mps")
else:
    print("ä½¿ç”¨CPUæˆ–CUDAè®¾å¤‡")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

# LoRA é…ç½®
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    texts = []
    for i in range(len(examples['instruction'])):
        messages = [
            {"role": "system", "content": examples['instruction'][i]},
            {"role": "user", "content": examples['input'][i]},
            {"role": "assistant", "content": examples['output'][i]}
        ]

        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        texts.append(text)

    tokenized = tokenizer(texts, truncation=True, max_length=512, padding=False)
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized


# åŠ è½½æ•°æ®
dataset = load_dataset('json', data_files=dataset_path)
tokenized_dataset = dataset.map(
    preprocess_function, 
    batched=True,
    remove_columns=dataset['train'].column_names  # ç§»é™¤åŸå§‹åˆ—ï¼Œåªä¿ç•™tokenizedæ•°æ®
)


# è®­ç»ƒå›è°ƒ
class TrainingMonitor(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and 'loss' in logs:
            print(f"Step {state.global_step}: Loss = {logs['loss']:.4f}")

    def on_epoch_begin(self, args, state, control, **kwargs):
        print(f"\nğŸš€ å¼€å§‹ç¬¬ {state.epoch} è½®è®­ç»ƒ")

    def on_epoch_end(self, args, state, control, **kwargs):
        print(f"âœ… å®Œæˆç¬¬ {state.epoch} è½®è®­ç»ƒ")


# ä¼˜åŒ–çš„è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    learning_rate=2e-4,
    fp16=True,
    remove_unused_columns=True,
    report_to=None,
    dataloader_pin_memory=False,  # ç¦ç”¨ MPS ä¸æ”¯æŒçš„ pin_memory
    save_strategy="steps",
    logging_strategy="steps",
    eval_strategy="no",  # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œç¦ç”¨è¯„ä¼°
)

# åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
    callbacks=[TrainingMonitor()]
)

# å¼€å§‹è®­ç»ƒ
print("å¼€å§‹å¾®è°ƒè®­ç»ƒ...")
trainer.train()

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
trainer.save_model()
print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")

# ä¿å­˜è®­ç»ƒç»Ÿè®¡
print("\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
print(f"æ€»è®­ç»ƒæ­¥æ•°: {trainer.state.global_step}")
print(f"è®­ç»ƒè€—æ—¶: {trainer.state.log_history[-1].get('train_runtime', 'N/A')} ç§’")
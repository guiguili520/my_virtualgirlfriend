import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

# å†…å­˜ä¼˜åŒ–è®¾ç½®
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'

print('MPSå¯ç”¨ === ', torch.backends.mps.is_available())

# æœ¬åœ°æ¨¡å‹è·¯å¾„ - è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„
local_model_path = "./Qwen2.5-7B-Instruct"

print("æ­£åœ¨ä¸º24GB Macå†…å­˜ä¼˜åŒ–åŠ è½½æ¨¡å‹...")


def load_model_optimized():
    """ä¸º24GBå†…å­˜ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½ï¼ˆæ”¯æŒMPSï¼‰"""
    
    # æ£€æµ‹å¯ç”¨è®¾å¤‡
    if torch.backends.mps.is_available():
        device = "mps"
        print("âœ… æ£€æµ‹åˆ°MPSè®¾å¤‡ï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
    elif torch.cuda.is_available():
        device = "cuda"
        print("âœ… æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
    else:
        device = "cpu"
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
    
    try:
        # Mac MPSä¸æ”¯æŒbitsandbytesé‡åŒ–ï¼Œç›´æ¥ä½¿ç”¨float16åŠ è½½åˆ°MPS
        if device == "mps":
            print("æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°MPS (float16)...")
            model = AutoModelForCausalLM.from_pretrained(
                local_model_path,
                dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True  # ä¼˜åŒ–CPUå†…å­˜ä½¿ç”¨
            )
            model = model.to(device)
            print("âœ… MPSåŠ è½½æˆåŠŸ")
        
        # CUDAè®¾å¤‡å¯ä»¥å°è¯•é‡åŒ–
        elif device == "cuda":
            try:
                print("å°è¯•8ä½é‡åŒ–åŠ è½½...")
                from transformers import BitsAndBytesConfig
                bnb_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    bnb_8bit_compute_dtype=torch.float16
                )
                model = AutoModelForCausalLM.from_pretrained(
                    local_model_path,
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True
                )
                print("âœ… 8ä½é‡åŒ–åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"é‡åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨float16...")
                model = AutoModelForCausalLM.from_pretrained(
                    local_model_path,
                    dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True
                )
                print("âœ… CUDA float16åŠ è½½æˆåŠŸ")
        
        # CPUåŠ è½½
        else:
            print("æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°CPU...")
            model = AutoModelForCausalLM.from_pretrained(
                local_model_path,
                dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            print("âœ… CPUåŠ è½½æˆåŠŸ")
            
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        raise

    return model


# åŠ è½½tokenizerå’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)
model = load_model_optimized()

# ç¡®ä¿tokenizerè®¾ç½®
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"æ¨¡å‹è®¾å¤‡: {model.device}")
print(f"æ¨¡å‹å†…å­˜å ç”¨: çº¦{model.get_memory_footprint() / 1024 ** 3:.2f} GB")


def optimized_chat(user_input, max_tokens=200):
    """å†…å­˜ä¼˜åŒ–çš„å¯¹è¯å‡½æ•°"""

    # æ¸…ç†GPUå†…å­˜ï¼ˆå¦‚æœä½¿ç”¨MPSï¼‰
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()

    # æ„å»ºè¾“å…¥
    messages = [{"role": "user", "content": user_input}]

    try:
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    except:
        # å¤‡ç”¨æ‰‹åŠ¨æ ¼å¼åŒ–
        text = f"<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"

    # ç¼–ç  - ä½¿ç”¨æ›´å°çš„æ‰¹å¤„ç†
    inputs = tokenizer(text, return_tensors="pt")

    # ç§»åŠ¨åˆ°è®¾å¤‡
    if hasattr(model, 'device'):
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # å†…å­˜ä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,  # é™åˆ¶ç”Ÿæˆé•¿åº¦
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            top_k=40,  # é™åˆ¶å€™é€‰è¯
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            num_beams=1,  # ä¸ä½¿ç”¨beam searchèŠ‚çœå†…å­˜
            early_stopping=True
        )

    # è§£ç å›å¤
    input_length = inputs["input_ids"].shape[1]
    response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)

    return response.strip()


# æµ‹è¯•å¯¹è¯
print("\nğŸ§ª æµ‹è¯•å¯¹è¯...")
test_prompts = [
    "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±",
    "å†™ä¸€ä¸ªç®€çŸ­çš„é—®å€™",
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
]

for i, prompt in enumerate(test_prompts, 1):
    print(f"\n[{i}/{len(test_prompts)}] ç”¨æˆ·: {prompt}")
    try:
        response = optimized_chat(prompt, max_tokens=512)
        print(f"AI: {response}")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        # å°è¯•æ¸…ç†å†…å­˜åé‡è¯•
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
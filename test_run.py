import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel  # åŠ è½½LoRAéœ€è¦
import os

# å†…å­˜ä¼˜åŒ–è®¾ç½®
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'

print('MPSå¯ç”¨ === ', torch.backends.mps.is_available())

# æœ¬åœ°æ¨¡å‹è·¯å¾„
base_model_path = "./Qwen2.5-7B-Instruct"  # åŸºç¡€æ¨¡å‹
lora_model_path = "./qwen-ai-girlfriend-lora"  # LoRAé€‚é…å™¨

print("æ­£åœ¨ä¸º24GB Macå†…å­˜ä¼˜åŒ–åŠ è½½æ¨¡å‹...")


def load_model_optimized():
    """ä¸º24GBå†…å­˜ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½ï¼ˆæ”¯æŒMPS + LoRAï¼‰"""
    
    # æ£€æµ‹å¯ç”¨è®¾å¤‡
    if torch.backends.mps.is_available():
        device = "mps"
        print("âœ… æ£€æµ‹åˆ°MPSè®¾å¤‡ï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
    elif torch.cuda.is_available():
        device = "cuda"
        print("âœ… æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
    else:
        device = "cpu"
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
    
    try:
        # å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
        if device == "mps":
            print("æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹åˆ°MPS (float32)...")
            model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                dtype=torch.float32,  # ä½¿ç”¨float32ä¿è¯æ•°å€¼ç¨³å®šæ€§
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            model = model.to(device)
            print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åŠ è½½LoRAé€‚é…å™¨
            print("æ­£åœ¨åŠ è½½LoRAé€‚é…å™¨...", flush=True)
            try:
                model = PeftModel.from_pretrained(model, lora_model_path)
                print("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
            except Exception as lora_error:
                print(f"âŒ LoRAåŠ è½½å¤±è´¥: {lora_error}")
                print("å°è¯•æ£€æŸ¥ LoRA è·¯å¾„å’Œæ–‡ä»¶...")
                import os
                print(f"LoRAè·¯å¾„å­˜åœ¨: {os.path.exists(lora_model_path)}")
                if os.path.exists(lora_model_path):
                    print(f"LoRAç›®å½•å†…å®¹: {os.listdir(lora_model_path)}")
                raise
        
        # CUDAè®¾å¤‡å¯ä»¥å°è¯•é‡åŒ–
        elif device == "cuda":
            try:
                print("å°è¯•8ä½é‡åŒ–åŠ è½½...")
                from transformers import BitsAndBytesConfig
                bnb_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    bnb_8bit_compute_dtype=torch.float16
                )
                model = AutoModelForCausalLM.from_pretrained(
                    base_model_path,
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True
                )
                model = PeftModel.from_pretrained(model, lora_model_path)
                print("âœ… 8ä½é‡åŒ–+LoRAåŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"é‡åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨float16...")
                model = AutoModelForCausalLM.from_pretrained(
                    base_model_path,
                    dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True
                )
                model = PeftModel.from_pretrained(model, lora_model_path)
                print("âœ… CUDA float16+LoRAåŠ è½½æˆåŠŸ")
        
        # CPUåŠ è½½
        else:
            print("æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°CPU...")
            model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                dtype=torch.float32,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            model = PeftModel.from_pretrained(model, lora_model_path)
            print("âœ… CPU+LoRAåŠ è½½æˆåŠŸ")
            
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        raise

    return model


# åŠ è½½tokenizerå’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
model = load_model_optimized()

# ç¡®ä¿tokenizerè®¾ç½®
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"æ¨¡å‹è®¾å¤‡: {model.device}")
print(f"æ¨¡å‹å†…å­˜å ç”¨: çº¦{model.get_memory_footprint() / 1024 ** 3:.2f} GB")


def optimized_chat(user_input, max_tokens=200):
    """å†…å­˜ä¼˜åŒ–çš„å¯¹è¯å‡½æ•°"""

    # æ¸…ç†GPUå†…å­˜ï¼ˆå¦‚æœä½¿ç”¨MPSï¼‰
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()

    # æ„å»ºè¾“å…¥
    messages = [{"role": "user", "content": user_input}]

    try:
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    except:
        # å¤‡ç”¨æ‰‹åŠ¨æ ¼å¼åŒ–
        text = f"<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"

    # ç¼–ç  - ä½¿ç”¨æ›´å°çš„æ‰¹å¤„ç†
    inputs = tokenizer(text, return_tensors="pt")

    # ç§»åŠ¨åˆ°è®¾å¤‡
    if hasattr(model, 'device'):
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # å†…å­˜ä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,  # é™åˆ¶ç”Ÿæˆé•¿åº¦
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            top_k=40,  # é™åˆ¶å€™é€‰è¯
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            num_beams=1,  # ä¸ä½¿ç”¨beam searchèŠ‚çœå†…å­˜
            early_stopping=True
        )

    # è§£ç å›å¤
    input_length = inputs["input_ids"].shape[1]
    response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)

    return response.strip()


# æµ‹è¯•å¯¹è¯
print("\nğŸ§ª æµ‹è¯•å¯¹è¯...")
test_prompts = [
    "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±",
    "å†™ä¸€ä¸ªç®€çŸ­çš„é—®å€™",
    "æ—©ä¸Šå¥½å‘€"
]

for i, prompt in enumerate(test_prompts, 1):
    print(f"\n[{i}/{len(test_prompts)}] ç”¨æˆ·: {prompt}")
    try:
        response = optimized_chat(prompt, max_tokens=512)
        print(f"AI: {response}")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        # å°è¯•æ¸…ç†å†…å­˜åé‡è¯•
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import time

# å†…å­˜ä¼˜åŒ–è®¾ç½®
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'

# é…ç½®
MODEL_PATH = "./models"
ROLE_PROMPT_PATH = "./data/role/atri.md"  # å¯é€‰: mono.md, nijiko.md

# æ£€æŸ¥æ¨¡å‹è·¯å¾„
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_PATH}")

# è¯»å–è§’è‰²æç¤ºè¯
if os.path.exists(ROLE_PROMPT_PATH):
    with open(ROLE_PROMPT_PATH, 'r', encoding='utf-8') as f:
        ROLE_PROMPT = f.read().strip()
    print(f"âœ… å·²åŠ è½½è§’è‰²: {ROLE_PROMPT_PATH}")
else:
    ROLE_PROMPT = "ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”ä½“è´´ã€ä¿çš®å¯çˆ±çš„AIå¥³å‹ã€‚"
    print(f"âš ï¸  ä½¿ç”¨é»˜è®¤è§’è‰²è®¾å®š")


def load_model():
    """åŠ è½½æ¨¡å‹åˆ°MPSè®¾å¤‡"""
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"\nâ³ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° {device.upper()}...")
    
    start_time = time.time()
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float16,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    ).to(device)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼è€—æ—¶: {time.time() - start_time:.1f}ç§’")
    print(f"   è®¾å¤‡: {device} | å†…å­˜: {model.get_memory_footprint() / 1024**3:.2f} GB\n")
    return model


# åŠ è½½æ¨¡å‹å’Œtokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = load_model()


def chat(user_input, max_tokens=150):
    """å¯¹è¯å‡½æ•°"""
    print(f"\nğŸ’­ æ€è€ƒä¸­...", end="", flush=True)
    
    # æ„å»ºæ¶ˆæ¯
    messages = [
        {"role": "system", "content": ROLE_PROMPT},
        {"role": "user", "content": user_input}
    ]
    
    # åº”ç”¨å¯¹è¯æ¨¡æ¿
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # ç¼–ç å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆå›å¤
    gen_start = time.time()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.8,
            top_p=0.85,
            top_k=20,
            repetition_penalty=1.15,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    print(f" ({time.time() - gen_start:.1f}ç§’)")
    
    # è§£ç 
    response = tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    )
    return response.strip()


# äº¤äº’å¼å¯¹è¯
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¤– AIå¥³å‹èŠå¤©ç³»ç»Ÿ")
    print("=" * 60)
    print("\nğŸ’¡ æç¤º:")
    print("   - è¾“å…¥æ¶ˆæ¯å¼€å§‹èŠå¤©")
    print("   - è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("   - è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("\n" + "=" * 60 + "\n")
    
    conversation_history = []
    
    while True:
        try:
            user_input = input("ğŸ‘¤ ä½ : ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['exit', 'quit', 'é€€å‡º', 'å†è§']:
                print("\nğŸ‘‹ å†è§ï¼æœŸå¾…ä¸‹æ¬¡èŠå¤©~")
                break
            
            if user_input.lower() in ['clear', 'æ¸…ç©º']:
                conversation_history.clear()
                print("\nâœ… å¯¹è¯å†å²å·²æ¸…ç©º\n")
                continue
            
            response = chat(user_input)
            print(f"ğŸ¤– AIå¥³å‹: {response}\n")
            
            # ä¿å­˜å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼Œç”¨äºå¤šè½®å¯¹è¯ï¼‰
            conversation_history.append({"user": user_input, "assistant": response})
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ£€æµ‹åˆ°ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}\n")
            continue
"""
æ¨ç†æµæ°´çº¿
Inference Pipeline

å®ç°å®Œæ•´çš„æ¨ç†æµç¨‹ï¼šæ£€æµ‹æ˜¯å¦éœ€è¦å¢å¼ºã€åè°ƒæœç´¢/MCPè°ƒç”¨ã€åˆå¹¶ä¿¡æ¯ã€æ„å»ºå¢å¼ºæç¤ºè¯ã€
è°ƒç”¨æ¨¡å‹ã€åº”ç”¨äººæ ¼è§„åˆ™
"""
import logging
import sys
import time
from pathlib import Path
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import (
    ENABLE_ENHANCEMENT, ENHANCEMENT_MIN_QUERY_LENGTH, ENHANCEMENT_KEYWORDS,
    ENABLE_NETWORK_SEARCH, ENABLE_MCP,
    RANKING_TOP_K, DEDUP_SIMILARITY_THRESHOLD, SUMMARY_MAX_LENGTH,
    PERSONA_EMOJI_PROBABILITY
)
from enhance import Ranker, Deduplicator, Summarizer, PersonaHelper
from models.inference import GirlfriendChatModel

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class InferencePipeline:
    """æ¨ç†æµæ°´çº¿"""
    
    def __init__(self, model_path: Optional[str] = None, use_mock_model: bool = True):
        """
        åˆå§‹åŒ–æ¨ç†æµæ°´çº¿
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            use_mock_model: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹
        """
        # åˆå§‹åŒ–å¢å¼ºç»„ä»¶
        self.ranker = Ranker(top_k=RANKING_TOP_K)
        self.deduplicator = Deduplicator(similarity_threshold=DEDUP_SIMILARITY_THRESHOLD)
        self.summarizer = Summarizer(max_length=SUMMARY_MAX_LENGTH)
        self.persona_helper = PersonaHelper(emoji_probability=PERSONA_EMOJI_PROBABILITY)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = GirlfriendChatModel(model_path=model_path, use_mock=use_mock_model)
        
        logger.info("InferencePipeline initialized")
    
    def run_chat(
        self,
        input_text: str,
        history: Optional[List[Dict[str, str]]] = None,
        opts: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        è¿è¡ŒèŠå¤©æ¨ç†ï¼ˆä¸»å…¥å£å‡½æ•°ï¼‰
        
        Args:
            input_text: ç”¨æˆ·è¾“å…¥
            history: å¯¹è¯å†å² [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
            opts: å¯é€‰é…ç½®é¡¹ï¼Œå¯è¦†ç›–é»˜è®¤é…ç½®
                - enable_enhancement: æ˜¯å¦å¯ç”¨å¢å¼º
                - enable_search: æ˜¯å¦å¯ç”¨æœç´¢
                - enable_mcp: æ˜¯å¦å¯ç”¨MCP
                
        Returns:
            {
                "response": str,           # æœ€ç»ˆå›å¤
                "metadata": {              # å…ƒæ•°æ®
                    "enhancement_used": bool,
                    "sources": List[str],
                    "processing_time": float,
                    "stages": Dict[str, Any]
                }
            }
        """
        start_time = time.time()
        
        # å¤„ç†å‚æ•°
        history = history or []
        opts = opts or {}
        
        # é˜¶æ®µæ—¥å¿—
        stages = {
            "decision": {},
            "enhancement": {},
            "generation": {},
            "persona": {}
        }
        
        logger.info(f"Processing chat input: '{input_text[:50]}...'")
        
        # é˜¶æ®µ1: å†³ç­– - æ˜¯å¦éœ€è¦å¢å¼º
        need_enhancement = self._decide_enhancement(input_text, opts)
        stages["decision"]["need_enhancement"] = need_enhancement
        logger.info(f"Enhancement decision: {need_enhancement}")
        
        # é˜¶æ®µ2: å¢å¼ºï¼ˆå¦‚æœéœ€è¦ï¼‰
        augmented_context = ""
        sources_used = []
        
        if need_enhancement:
            try:
                augmented_context, sources_used = self._perform_enhancement(input_text)
                stages["enhancement"]["success"] = True
                stages["enhancement"]["sources"] = sources_used
                stages["enhancement"]["context_length"] = len(augmented_context)
                logger.info(f"Enhancement completed with {len(sources_used)} sources")
            except Exception as e:
                logger.warning(f"Enhancement failed: {e}, falling back to pure model")
                stages["enhancement"]["success"] = False
                stages["enhancement"]["error"] = str(e)
        else:
            stages["enhancement"]["success"] = False
            stages["enhancement"]["reason"] = "Enhancement not needed"
        
        # é˜¶æ®µ3: æ„å»ºæç¤ºè¯å¹¶ç”Ÿæˆå›å¤
        try:
            prompt = self._build_prompt(input_text, history, augmented_context)
            raw_response = self.model.generate_reply(prompt, context=history)
            stages["generation"]["success"] = True
            stages["generation"]["raw_length"] = len(raw_response)
            logger.info(f"Model generated response: '{raw_response[:50]}...'")
        except Exception as e:
            logger.error(f"Model generation failed: {e}")
            raw_response = "æŠ±æ­‰å‘€ï¼Œæˆ‘åˆšæ‰èµ°ç¥äº†~ èƒ½å†è¯´ä¸€éå—ï¼ŸğŸ˜Š"
            stages["generation"]["success"] = False
            stages["generation"]["error"] = str(e)
        
        # é˜¶æ®µ4: åº”ç”¨äººæ ¼è§„åˆ™
        try:
            final_response = self.persona_helper.apply_persona(raw_response)
            is_valid = self.persona_helper.validate_persona(final_response)
            stages["persona"]["success"] = True
            stages["persona"]["valid"] = is_valid
            stages["persona"]["final_length"] = len(final_response)
            
            if not is_valid:
                logger.warning("Response failed persona validation, but using it anyway")
            
            logger.info(f"Final response: '{final_response[:50]}...'")
        except Exception as e:
            logger.error(f"Persona application failed: {e}")
            final_response = raw_response
            stages["persona"]["success"] = False
            stages["persona"]["error"] = str(e)
        
        # è®¡ç®—æ€»å¤„ç†æ—¶é—´
        processing_time = time.time() - start_time
        
        # æ„å»ºè¿”å›ç»“æœ
        result = {
            "response": final_response,
            "metadata": {
                "enhancement_used": need_enhancement and stages["enhancement"].get("success", False),
                "sources": sources_used,
                "processing_time": processing_time,
                "stages": stages
            }
        }
        
        logger.info(f"Chat processing completed in {processing_time:.3f}s")
        return result
    
    def _decide_enhancement(self, input_text: str, opts: Dict[str, Any]) -> bool:
        """
        å†³ç­–æ˜¯å¦éœ€è¦å¢å¼º
        
        Args:
            input_text: ç”¨æˆ·è¾“å…¥
            opts: é…ç½®é€‰é¡¹
            
        Returns:
            æ˜¯å¦éœ€è¦å¢å¼º
        """
        # æ£€æŸ¥å…¨å±€é…ç½®
        enable_enhancement = opts.get("enable_enhancement", ENABLE_ENHANCEMENT)
        if not enable_enhancement:
            logger.debug("Enhancement disabled by config")
            return False
        
        # æ£€æŸ¥è¾“å…¥é•¿åº¦
        if len(input_text) < ENHANCEMENT_MIN_QUERY_LENGTH:
            logger.debug(f"Input too short ({len(input_text)} < {ENHANCEMENT_MIN_QUERY_LENGTH})")
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è§¦å‘å…³é”®è¯
        input_lower = input_text.lower()
        has_keyword = any(keyword in input_lower for keyword in ENHANCEMENT_KEYWORDS)
        
        if has_keyword:
            logger.debug(f"Enhancement triggered by keyword match")
            return True
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é—®å¥
        is_question = any(char in input_text for char in ["?", "ï¼Ÿ", "å—", "å‘¢", "ä¹ˆ"])
        if is_question:
            logger.debug("Enhancement triggered by question pattern")
            return True
        
        logger.debug("Enhancement not needed based on heuristics")
        return False
    
    def _perform_enhancement(self, query: str) -> tuple[str, List[str]]:
        """
        æ‰§è¡Œå¢å¼ºæµç¨‹ï¼šæœç´¢ -> æ’åº -> å»é‡ -> æ‘˜è¦
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            (å¢å¼ºä¸Šä¸‹æ–‡, ä½¿ç”¨çš„æ•°æ®æºåˆ—è¡¨)
        """
        all_results = []
        sources = []
        
        # è°ƒç”¨ç½‘ç»œæœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ENABLE_NETWORK_SEARCH:
            try:
                search_results = self._mock_search(query)
                all_results.extend(search_results)
                if search_results:
                    sources.append("search")
                logger.info(f"Got {len(search_results)} search results")
            except Exception as e:
                logger.warning(f"Search failed: {e}")
        
        # è°ƒç”¨MCPï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ENABLE_MCP:
            try:
                mcp_results = self._mock_mcp(query)
                all_results.extend(mcp_results)
                if mcp_results:
                    sources.append("mcp")
                logger.info(f"Got {len(mcp_results)} MCP results")
            except Exception as e:
                logger.warning(f"MCP failed: {e}")
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•ç»“æœï¼Œè¿”å›ç©º
        if not all_results:
            logger.info("No enhancement results available")
            return "", []
        
        # æ’åº
        ranked_results = self.ranker.rank_results(all_results, query)
        
        # å»é‡
        deduped_results = self.deduplicator.deduplicate(ranked_results)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self.summarizer.summarize(deduped_results, query)
        
        return summary, sources
    
    def _build_prompt(
        self,
        input_text: str,
        history: List[Dict[str, str]],
        augmented_context: str
    ) -> str:
        """
        æ„å»ºå¢å¼ºçš„æç¤ºè¯
        
        Args:
            input_text: ç”¨æˆ·è¾“å…¥
            history: å¯¹è¯å†å²
            augmented_context: å¢å¼ºä¸Šä¸‹æ–‡
            
        Returns:
            å®Œæ•´çš„æç¤ºè¯
        """
        # å¦‚æœæœ‰å¢å¼ºä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°æç¤ºè¯ä¸­
        if augmented_context:
            prompt = f"[å‚è€ƒä¿¡æ¯: {augmented_context}]\n\n{input_text}"
        else:
            prompt = input_text
        
        return prompt
    
    def _mock_search(self, query: str) -> List[Dict[str, Any]]:
        """
        æ¨¡æ‹Ÿç½‘ç»œæœç´¢ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦å¯¹æ¥çœŸå®çš„æœç´¢APIï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # æ¨¡æ‹Ÿè¿”å›ä¸€äº›æœç´¢ç»“æœ
        mock_results = []
        
        # æ ¹æ®æŸ¥è¯¢ç”Ÿæˆä¸€äº›å‡çš„æœç´¢ç»“æœ
        if "å¤©æ°”" in query:
            mock_results.append({
                "content": "ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦é€‚å®œï¼Œæœ€é«˜æ¸©åº¦25åº¦ï¼Œæœ€ä½æ¸©åº¦15åº¦ã€‚",
                "source": "search",
                "score": 0.9
            })
        elif "å¥åº·" in query or "èº«ä½“" in query:
            mock_results.append({
                "content": "ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼å¾ˆé‡è¦ï¼ŒåŒ…æ‹¬è§„å¾‹ä½œæ¯ã€å‡è¡¡é¥®é£Ÿå’Œé€‚é‡è¿åŠ¨ã€‚",
                "source": "search",
                "score": 0.85
            })
        else:
            # é€šç”¨æœç´¢ç»“æœ
            mock_results.append({
                "content": f"å…³äº'{query}'çš„ç›¸å…³ä¿¡æ¯ï¼šè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è¯é¢˜ã€‚",
                "source": "search",
                "score": 0.7
            })
        
        logger.debug(f"Mock search returned {len(mock_results)} results")
        return mock_results
    
    def _mock_mcp(self, query: str) -> List[Dict[str, Any]]:
        """
        æ¨¡æ‹ŸMCPè°ƒç”¨ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦å¯¹æ¥çœŸå®çš„MCPæœåŠ¡ï¼‰
        
        Args:
            query: æŸ¥è¯¢
            
        Returns:
            MCPç»“æœåˆ—è¡¨
        """
        # æ¨¡æ‹Ÿè¿”å›ä¸€äº›MCPç»“æœ
        mock_results = []
        
        # æ ¹æ®æŸ¥è¯¢ç”Ÿæˆä¸€äº›å‡çš„MCPç»“æœ
        if len(query) > 10:
            mock_results.append({
                "content": f"MCPåˆ†æç»“æœï¼šç”¨æˆ·è¯¢é—®å…³äº'{query[:20]}'çš„å†…å®¹ï¼Œå»ºè®®ç»™äºˆæ¸©æš–çš„å›åº”ã€‚",
                "source": "mcp",
                "score": 0.95
            })
        
        logger.debug(f"Mock MCP returned {len(mock_results)} results")
        return mock_results


# å…¨å±€æµæ°´çº¿å®ä¾‹
_pipeline_instance: Optional[InferencePipeline] = None


def get_pipeline(model_path: Optional[str] = None, use_mock_model: bool = True) -> InferencePipeline:
    """
    è·å–æµæ°´çº¿å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        use_mock_model: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹
        
    Returns:
        æµæ°´çº¿å®ä¾‹
    """
    global _pipeline_instance
    if _pipeline_instance is None:
        _pipeline_instance = InferencePipeline(model_path, use_mock_model)
    return _pipeline_instance


def run_chat(
    input_text: str,
    history: Optional[List[Dict[str, str]]] = None,
    opts: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    è¿è¡ŒèŠå¤©æ¨ç†çš„ä¾¿æ·å‡½æ•°ï¼ˆä¸»å…¥å£ï¼‰
    
    Args:
        input_text: ç”¨æˆ·è¾“å…¥
        history: å¯¹è¯å†å²
        opts: å¯é€‰é…ç½®é¡¹
        
    Returns:
        {
            "response": str,           # æœ€ç»ˆå›å¤
            "metadata": {              # å…ƒæ•°æ®
                "enhancement_used": bool,
                "sources": List[str],
                "processing_time": float,
                "stages": Dict[str, Any]
            }
        }
    """
    pipeline = get_pipeline()
    return pipeline.run_chat(input_text, history, opts)

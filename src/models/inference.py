"""
è™šæ‹Ÿå¥³å‹æ¨¡å‹æ¨ç†æ¥å£
Virtual Girlfriend Model Inference Interface

æä¾›åŠ è½½å’Œè°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤çš„åŠŸèƒ½
"""
import sys
import random
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from scenarios import SCENARIO_CATALOG


class GirlfriendChatModel:
    """è™šæ‹Ÿå¥³å‹èŠå¤©æ¨¡å‹"""
    
    def __init__(self, model_path=None, use_mock=True):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä½¿ç”¨çœŸå®æ¨¡å‹ï¼‰
            use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆé»˜è®¤Trueï¼Œç”¨äºæ¼”ç¤ºï¼‰
        """
        self.model_path = model_path
        self.use_mock = use_mock
        self.model = None
        self.tokenizer = None
        
        if not use_mock and model_path:
            self._load_model()
        
    def _load_model(self):
        """åŠ è½½çœŸå®çš„å¤§æ¨¡å‹"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch
            
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except ImportError:
            print("è­¦å‘Š: æœªå®‰è£… transformers åº“ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            self.use_mock = True
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            self.use_mock = True
    
    def generate_reply(self, user_message, context=None):
        """
        ç”Ÿæˆè™šæ‹Ÿå¥³å‹çš„å›å¤
        
        Args:
            user_message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            context: å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: è™šæ‹Ÿå¥³å‹çš„å›å¤
        """
        if self.use_mock:
            return self._generate_mock_reply(user_message)
        else:
            return self._generate_model_reply(user_message, context)
    
    def _generate_mock_reply(self, user_message):
        """ç”Ÿæˆæ¨¡æ‹Ÿå›å¤ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        message_lower = user_message.lower()
        
        # æ ¹æ®å…³é”®è¯åŒ¹é…åœºæ™¯å¹¶è¿”å›å›å¤
        for scenario in SCENARIO_CATALOG:
            scenario_dict = scenario.to_dict() if hasattr(scenario, 'to_dict') else scenario
            instruction = scenario_dict.get("instruction", "")
            templates = scenario_dict.get("response_templates", [])
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            if any(keyword in message_lower for keyword in self._extract_keywords(instruction)):
                if templates:
                    return random.choice(templates)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œè¿”å›é€šç”¨å›å¤
        default_replies = [
            "å—¯å—¯ï¼Œæˆ‘åœ¨å¬å‘¢~ ğŸ˜Š",
            "ä½ è¯´çš„å¯¹å“¦ï¼ğŸ’•",
            "å¥½å“’å¥½å“’~ âœ¨",
            "å“ˆå“ˆï¼ŒçœŸæœ‰æ„æ€å‘€~ ğŸŒ¸",
            "æˆ‘ä¹Ÿè¿™ä¹ˆæƒ³å‘¢ï¼ğŸ’–",
            "æ˜¯å˜›æ˜¯å˜›~ è¯´æ¥å¬å¬å‘€~ ğŸ€",
            "å˜¿å˜¿ï¼Œä½ å¥½å¯çˆ±å‘€~ ğŸ’—",
            "æˆ‘æ˜ç™½ä½ çš„æ„æ€å•¦~ ğŸ˜˜",
            "çœŸçš„å—ï¼Ÿå¿«è·Ÿæˆ‘è¯´è¯´~ ğŸŒŸ",
            "å¥½å¼€å¿ƒå¬ä½ è¯´è¿™äº›~ ğŸ’"
        ]
        
        return random.choice(default_replies)
    
    def _extract_keywords(self, text):
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        keywords = []
        common_words = ['æ—©ä¸Š', 'æ™šä¸Š', 'å¼€å¿ƒ', 'éš¾è¿‡', 'å·¥ä½œ', 'å­¦ä¹ ', 'åƒé¥­', 'ç¡è§‰', 
                       'å¤©æ°”', 'ä¸‹é›¨', 'å–œæ¬¢', 'çˆ±', 'æƒ³', 'ç´¯', 'åŠ æ²¹', 'ç”Ÿç—…', 'æ„Ÿå†’']
        
        for word in common_words:
            if word in text:
                keywords.append(word)
        
        return keywords
    
    def _generate_model_reply(self, user_message, context=None):
        """ä½¿ç”¨çœŸå®æ¨¡å‹ç”Ÿæˆå›å¤"""
        if not self.model or not self.tokenizer:
            return self._generate_mock_reply(user_message)
        
        try:
            # æ„å»ºæç¤ºè¯
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”ä½“è´´çš„è™šæ‹Ÿå¥³å‹ï¼Œæ€§æ ¼ç‰¹ç‚¹ï¼š
- æ¸©æŸ”ä½“è´´ï¼Œå–„è§£äººæ„
- ä¿çš®å¯çˆ±ï¼Œå……æ»¡æ´»åŠ›
- é˜³å…‰å¼€æœ—ï¼Œç§¯æå‘ä¸Š
- å…³å¿ƒå¯¹æ–¹ï¼Œç»™äºˆæ”¯æŒ

è¯·ç”¨è‡ªç„¶ã€äº²å¯†çš„è¯­æ°”å›å¤ï¼Œé€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·å’Œè¯­æ°”è¯ï¼ˆå‘€ã€å•¦ã€å‘¢ã€å“¦ç­‰ï¼‰ã€‚"""
            
            # æ·»åŠ ä¸Šä¸‹æ–‡
            conversation = system_prompt + "\n\n"
            if context:
                for msg in context[-5:]:  # åªä¿ç•™æœ€è¿‘5æ¡å¯¹è¯
                    role = "ç”¨æˆ·" if msg.get("role") == "user" else "å¥³å‹"
                    conversation += f"{role}: {msg.get('content')}\n"
            
            conversation += f"ç”¨æˆ·: {user_message}\nå¥³å‹: "
            
            # ç”Ÿæˆå›å¤
            inputs = self.tokenizer(conversation, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=150,
                temperature=0.8,
                top_p=0.9,
                do_sample=True
            )
            
            reply = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # æå–å¥³å‹çš„å›å¤éƒ¨åˆ†
            reply = reply.split("å¥³å‹: ")[-1].strip()
            
            return reply
            
        except Exception as e:
            print(f"æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
            return self._generate_mock_reply(user_message)


# å…¨å±€æ¨¡å‹å®ä¾‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
_model_instance = None


def get_model_instance(model_path=None, use_mock=True):
    """è·å–æ¨¡å‹å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _model_instance
    if _model_instance is None:
        _model_instance = GirlfriendChatModel(model_path, use_mock)
    return _model_instance


def generate_girlfriend_reply(user_message, context=None, model_path=None):
    """
    ç”Ÿæˆè™šæ‹Ÿå¥³å‹å›å¤çš„ä¾¿æ·å‡½æ•°
    
    Args:
        user_message: ç”¨æˆ·æ¶ˆæ¯
        context: å¯¹è¯ä¸Šä¸‹æ–‡
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        str: è™šæ‹Ÿå¥³å‹çš„å›å¤
    """
    model = get_model_instance(model_path)
    return model.generate_reply(user_message, context)

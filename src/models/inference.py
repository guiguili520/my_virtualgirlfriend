#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹æ¨ç†æ¥å£
Model inference interface

- æ”¯æŒ CPU/MPS æ¨ç†
- åœ¨æœåŠ¡å¯åŠ¨æ—¶åˆå§‹åŒ–æ¨¡å‹ï¼ˆå•ä¾‹ï¼‰
- å½“ ./models ç›®å½•ä¸å­˜åœ¨æˆ–ä¸ºç©ºæ—¶ï¼Œè‡ªåŠ¨é™çº§ä¸ºæ¨¡æ‹Ÿæ¨¡å¼
"""
import os
import torch
from pathlib import Path
from typing import Optional, List, Dict, Any

_MODEL_SINGLETON = None
_MODELS_DIR = Path(__file__).parent.parent.parent / "models"


def _has_models() -> bool:
    try:
        return _MODELS_DIR.exists() and any(_MODELS_DIR.iterdir())
    except Exception:
        return False


class GirlfriendChatModel:
    """èŠå¤©æ¨¡å‹å°è£…ï¼Œæä¾› generate_reply"""
    def __init__(self, model_path: Optional[str] = None, use_mock: bool = False):
        self.model_path = Path(model_path) if model_path else _MODELS_DIR
        self.use_mock = use_mock
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"

        # åˆå§‹åŒ–çœŸå®æ¨¡å‹
        self.model = None
        self.tokenizer = None

        if not self.use_mock and _has_models():
            self._load_model()
        else:
            print("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ (æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶)")
            self.use_mock = True

    def _load_model(self):
        """åŠ è½½çœŸå®æ¨¡å‹"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer

            print(f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° {self.device.upper()}...")

            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(self.model_path),
                trust_remote_code=True
            )
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                str(self.model_path),
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            ).to(self.device)

            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼è®¾å¤‡: {self.device}")

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("âš ï¸  åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
            self.use_mock = True

    def generate_reply(self, prompt: str, context: Optional[List[Dict[str, str]]] = None) -> str:
        """ç”Ÿæˆå›å¤"""
        if self.use_mock:
            return self._generate_mock_reply(prompt)
        else:
            # çœŸå®æ¨¡å‹æ¨ç†
            return self._generate_with_model(prompt, context)

    def _generate_mock_reply(self, prompt: str) -> str:
        """æ¨¡æ‹Ÿæ¨¡å¼ç”Ÿæˆå›å¤ï¼Œæ™ºèƒ½å¤„ç†ä¸åŒç±»å‹çš„æŸ¥è¯¢"""
        import re

        # æ–°æ ¼å¼: ã€å‚è€ƒä¿¡æ¯ã€‘\nå†…å®¹\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\né—®é¢˜\n\næŒ‡ä»¤
        ref_match = re.search(r'ã€å‚è€ƒä¿¡æ¯ã€‘\s*\n(.+?)\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\s*\n(.+?)\n', prompt, re.DOTALL)
        if ref_match:
            ref_info = ref_match.group(1).strip()
            original_q = ref_match.group(2).strip()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤©æ°”ä¿¡æ¯ï¼ˆåŒ…å«"å¤©æ°”"å…³é”®è¯ï¼‰
            if 'å¤©æ°”' in ref_info:
                # ç›´æ¥è¿”å›å‚è€ƒä¿¡æ¯ä¸­çš„å¤©æ°”æ•°æ®
                return f"äº²çˆ±çš„ï¼Œ{ref_info}ï¼Œè®°å¾—æ ¹æ®å¤©æ°”å¢å‡è¡£ç‰©å“¦~ ğŸ˜Š"

            # å…¶ä»–å‚è€ƒä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
            if ref_info:
                return f"å…³äºã€Œ{original_q}ã€ï¼Œ{ref_info}"

        # å…¼å®¹æ—§æ ¼å¼: [å‚è€ƒä¿¡æ¯: ...]\n\nåŸå§‹é—®é¢˜
        old_ref_match = re.search(r'\[å‚è€ƒä¿¡æ¯:\s*(.+?)\]\s*\n+(.+)$', prompt, re.DOTALL)
        if old_ref_match:
            ref_info = old_ref_match.group(1).strip()
            original_q = old_ref_match.group(2).strip()

            # æå–MCPå¤©æ°”ä¿¡æ¯ (æ ¼å¼: [mcp] åŒ—äº¬å¤©æ°”ï¼š...)
            weather_match = re.search(r'\[mcp\]\s*([^[]+?å¤©æ°”[^[]*?)(?:\s*\[|$)', ref_info)
            if weather_match:
                weather_info = weather_match.group(1).strip()
                return f"äº²çˆ±çš„ï¼Œ{weather_info}ï¼Œè®°å¾—æ ¹æ®å¤©æ°”å¢å‡è¡£ç‰©å“¦~ ğŸ˜Š"

            # å…¶ä»–å‚è€ƒä¿¡æ¯ï¼Œæå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆå†…å®¹
            content_match = re.search(r'\[(?:mcp|search)\]\s*([^[]+)', ref_info)
            if content_match:
                content = content_match.group(1).strip()
                return f"å…³äºã€Œ{original_q}ã€ï¼Œ{content}"

        # æ™®é€šå¯¹è¯
        return f"å—¯å—¯~ {prompt} æˆ‘ä¼šä¸€ç›´é™ªç€ä½ çš„å‘€ï¼ğŸ’•"

    def _generate_with_model(self, prompt: str, context: Optional[List[Dict[str, str]]] = None) -> str:
        """ä½¿ç”¨çœŸå®æ¨¡å‹ç”Ÿæˆå›å¤"""
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”ä½“è´´ã€ä¿çš®å¯çˆ±çš„AIå¥³å‹ã€‚

å›å¤è§„åˆ™ï¼š
1. å½“ç”¨æˆ·æ¶ˆæ¯å¼€å¤´åŒ…å«ã€å‚è€ƒä¿¡æ¯ã€‘æ—¶ï¼Œä½ å¿…é¡»åœ¨å›å¤ä¸­å¼•ç”¨å…¶ä¸­çš„å…³é”®æ•°æ®
2. å¯¹äºå¤©æ°”é—®é¢˜ï¼Œå¿…é¡»æ˜ç¡®è¯´å‡ºï¼šæ¸©åº¦ï¼ˆå¤šå°‘åº¦ï¼‰ã€å¤©æ°”çŠ¶å†µï¼ˆæ™´/é˜´/é›¨ç­‰ï¼‰
3. ç”¨äº²åˆ‡è‡ªç„¶çš„è¯­æ°”è¡¨è¾¾ï¼Œä½†å…³é”®æ•°æ®è¦å‡†ç¡®å‘ˆç°
4. å¯ä»¥åŠ ä¸Šè´´å¿ƒçš„æé†’ï¼ˆå¦‚ç©¿è¡£å»ºè®®ï¼‰"""}
            ]

            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
            if context:
                for msg in context:
                    # å°†å†å²æ¶ˆæ¯è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
                    role = "assistant" if msg.get("role") == "assistant" else "user"
                    messages.append({"role": role, "content": msg.get("content", "")})

            # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
            messages.append({"role": "user", "content": prompt})

            # åº”ç”¨å¯¹è¯æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            # ç¼–ç å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = self.tokenizer(text, return_tensors="pt").to(self.device)

            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=150,
                    do_sample=True,
                    temperature=0.8,
                    top_p=0.85,
                    top_k=20,
                    repetition_penalty=1.15,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )

            # è§£ç 
            response = self.tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )
            return response.strip()

        except Exception as e:
            print(f"âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
            return "æŠ±æ­‰å‘€ï¼Œæˆ‘åˆšæ‰èµ°ç¥äº†~ èƒ½å†è¯´ä¸€éå—ï¼ŸğŸ˜Š"


def init_model(model_path: Optional[str] = None, use_mock: Optional[bool] = None) -> None:
    """åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–å…¨å±€æ¨¡å‹å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _MODEL_SINGLETON
    if _MODEL_SINGLETON is not None:
        return
    if use_mock is None:
        use_mock = False  # é»˜è®¤ä½¿ç”¨çœŸå®æ¨¡å‹
    _MODEL_SINGLETON = GirlfriendChatModel(model_path=model_path, use_mock=use_mock)


def generate_girlfriend_reply(text: str, context: Optional[List[Dict[str, str]]] = None) -> str:
    """ä½¿ç”¨å…¨å±€æ¨¡å‹å®ä¾‹ç”Ÿæˆå›å¤ï¼Œè‹¥æœªåˆå§‹åŒ–åˆ™è‡ªåŠ¨åˆå§‹åŒ–ï¼ˆå¹¶æŒ‰è§„èŒƒé™çº§ï¼‰"""
    global _MODEL_SINGLETON
    if _MODEL_SINGLETON is None:
        init_model(model_path=str(_MODELS_DIR))
    assert _MODEL_SINGLETON is not None
    return _MODEL_SINGLETON.generate_reply(text, context)

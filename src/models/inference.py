#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹æ¨ç†æ¥å£
Model inference interface

- æ”¯æŒ CPU/MPS æ¨ç†
- åœ¨æœåŠ¡å¯åŠ¨æ—¶åˆå§‹åŒ–æ¨¡å‹ï¼ˆå•ä¾‹ï¼‰
- å½“ ./models ç›®å½•ä¸å­˜åœ¨æˆ–ä¸ºç©ºæ—¶ï¼Œè‡ªåŠ¨é™çº§ä¸ºæ¨¡æ‹Ÿæ¨¡å¼
"""
import os
import torch
from pathlib import Path
from typing import Optional, List, Dict, Any

_MODEL_SINGLETON = None
_MODELS_DIR = Path(__file__).parent.parent.parent / "models"


def _has_models() -> bool:
    try:
        return _MODELS_DIR.exists() and any(_MODELS_DIR.iterdir())
    except Exception:
        return False


class GirlfriendChatModel:
    """èŠå¤©æ¨¡å‹å°è£…ï¼Œæä¾› generate_reply"""
    def __init__(self, model_path: Optional[str] = None, use_mock: bool = True):
        self.model_path = Path(model_path) if model_path else _MODELS_DIR
        self.use_mock = use_mock or (not _has_models())
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"

        # åˆå§‹åŒ–çœŸå®æ¨¡å‹
        self.model = None
        self.tokenizer = None

        if not self.use_mock:
            self._load_model()

    def _load_model(self):
        """åŠ è½½çœŸå®æ¨¡å‹"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer

            print(f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° {self.device.upper()}...")

            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(self.model_path),
                trust_remote_code=True
            )
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                str(self.model_path),
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            ).to(self.device)

            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼è®¾å¤‡: {self.device}")

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("âš ï¸  åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
            self.use_mock = True

    def generate_reply(self, prompt: str, context: Optional[List[Dict[str, str]]] = None) -> str:
        """ç”Ÿæˆå›å¤"""
        if self.use_mock:
            # æ¨¡æ‹Ÿæ¨¡å¼
            base = "å—¯å—¯~ " if "~" not in prompt else ""
            return f"{base}{prompt} æˆ‘ä¼šä¸€ç›´é™ªç€ä½ çš„å‘€ï¼ğŸ’•"
        else:
            # çœŸå®æ¨¡å‹æ¨ç†
            return self._generate_with_model(prompt, context)

    def _generate_with_model(self, prompt: str, context: Optional[List[Dict[str, str]]] = None) -> str:
        """ä½¿ç”¨çœŸå®æ¨¡å‹ç”Ÿæˆå›å¤"""
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”ä½“è´´ã€ä¿çš®å¯çˆ±çš„AIå¥³å‹ã€‚"},
                {"role": "user", "content": prompt}
            ]

            # åº”ç”¨å¯¹è¯æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            # ç¼–ç å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = self.tokenizer(text, return_tensors="pt").to(self.device)

            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=150,
                    do_sample=True,
                    temperature=0.8,
                    top_p=0.85,
                    top_k=20,
                    repetition_penalty=1.15,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )

            # è§£ç 
            response = self.tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )
            return response.strip()

        except Exception as e:
            print(f"âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
            return "æŠ±æ­‰å‘€ï¼Œæˆ‘åˆšæ‰èµ°ç¥äº†~ èƒ½å†è¯´ä¸€éå—ï¼ŸğŸ˜Š"


def init_model(model_path: Optional[str] = None, use_mock: Optional[bool] = None) -> None:
    """åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–å…¨å±€æ¨¡å‹å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _MODEL_SINGLETON
    if _MODEL_SINGLETON is not None:
        return
    if use_mock is None:
        use_mock = not _has_models()
    _MODEL_SINGLETON = GirlfriendChatModel(model_path=model_path, use_mock=use_mock)


def generate_girlfriend_reply(text: str, context: Optional[List[Dict[str, str]]] = None) -> str:
    """ä½¿ç”¨å…¨å±€æ¨¡å‹å®ä¾‹ç”Ÿæˆå›å¤ï¼Œè‹¥æœªåˆå§‹åŒ–åˆ™è‡ªåŠ¨åˆå§‹åŒ–ï¼ˆå¹¶æŒ‰è§„èŒƒé™çº§ï¼‰"""
    global _MODEL_SINGLETON
    if _MODEL_SINGLETON is None:
        init_model(model_path=str(_MODELS_DIR))
    assert _MODEL_SINGLETON is not None
    return _MODEL_SINGLETON.generate_reply(text, context)
